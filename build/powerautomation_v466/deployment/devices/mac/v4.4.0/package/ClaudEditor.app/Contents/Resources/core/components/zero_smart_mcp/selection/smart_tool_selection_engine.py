#!/usr/bin/env python3
"""
Smart Tool Engineæ™ºèƒ½é€‰æ‹©å±‚

åŸºäºAIé©±åŠ¨çš„æ™ºèƒ½å·¥å…·é€‰æ‹©å¼•æ“ï¼Œèƒ½å¤Ÿæ ¹æ®ä»»åŠ¡éœ€æ±‚è‡ªåŠ¨æ¨èæœ€é€‚åˆçš„MCPå·¥å…·ã€‚
æ”¯æŒå¤šç§é€‰æ‹©ç­–ç•¥ã€æˆæœ¬ä¼˜åŒ–ã€æ€§èƒ½é¢„æµ‹å’Œå­¦ä¹ åé¦ˆæœºåˆ¶ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
- AIé©±åŠ¨çš„å·¥å…·æ¨è
- å¤šç»´åº¦å·¥å…·è¯„åˆ†
- æˆæœ¬æ•ˆç›Šåˆ†æ
- æ€§èƒ½é¢„æµ‹
- ç”¨æˆ·åå¥½å­¦ä¹ 
- å·¥å…·ç»„åˆä¼˜åŒ–

ä½œè€…: PowerAutomation Team
ç‰ˆæœ¬: 4.1.0
æ—¥æœŸ: 2025-01-07
"""

import asyncio
import json
import uuid
import numpy as np
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass, field, asdict
from enum import Enum
import pickle
import hashlib
from pathlib import Path
import math

from ..models.tool_models import MCPTool, TaskRequirement, ToolRecommendation, ToolCapability, ToolStatus

logger = logging.getLogger(__name__)

class SelectionStrategy(Enum):
    """é€‰æ‹©ç­–ç•¥"""
    BEST_MATCH = "best_match"  # æœ€ä½³åŒ¹é…
    COST_OPTIMIZED = "cost_optimized"  # æˆæœ¬ä¼˜åŒ–
    PERFORMANCE_FIRST = "performance_first"  # æ€§èƒ½ä¼˜å…ˆ
    BALANCED = "balanced"  # å¹³è¡¡ç­–ç•¥
    USER_PREFERENCE = "user_preference"  # ç”¨æˆ·åå¥½
    ENSEMBLE = "ensemble"  # é›†æˆç­–ç•¥

class RecommendationReason(Enum):
    """æ¨èåŸå› """
    CAPABILITY_MATCH = "capability_match"
    PERFORMANCE_SCORE = "performance_score"
    COST_EFFICIENCY = "cost_efficiency"
    USER_HISTORY = "user_history"
    POPULARITY = "popularity"
    COMPATIBILITY = "compatibility"
    NOVELTY = "novelty"

@dataclass
class ToolScore:
    """å·¥å…·è¯„åˆ†"""
    tool_id: str
    overall_score: float
    capability_score: float
    performance_score: float
    cost_score: float
    popularity_score: float
    compatibility_score: float
    user_preference_score: float
    confidence: float
    reasons: List[RecommendationReason] = field(default_factory=list)
    details: Dict[str, Any] = field(default_factory=dict)

@dataclass
class UserProfile:
    """ç”¨æˆ·ç”»åƒ"""
    user_id: str
    preferences: Dict[str, float] = field(default_factory=dict)
    usage_history: List[Dict[str, Any]] = field(default_factory=list)
    favorite_tools: Set[str] = field(default_factory=set)
    avoided_tools: Set[str] = field(default_factory=set)
    skill_level: str = "intermediate"  # beginner, intermediate, advanced
    domain_expertise: List[str] = field(default_factory=list)
    cost_sensitivity: float = 0.5  # 0-1, è¶Šé«˜è¶Šæ•æ„Ÿ
    performance_priority: float = 0.7  # 0-1, è¶Šé«˜è¶Šé‡è§†æ€§èƒ½
    learning_rate: float = 0.1  # å­¦ä¹ é€Ÿç‡
    last_updated: datetime = field(default_factory=datetime.now)

@dataclass
class ToolPerformanceMetrics:
    """å·¥å…·æ€§èƒ½æŒ‡æ ‡"""
    tool_id: str
    success_rate: float = 0.0
    average_execution_time: float = 0.0
    error_rate: float = 0.0
    user_satisfaction: float = 0.0
    usage_frequency: int = 0
    last_used: Optional[datetime] = None
    performance_trend: float = 0.0  # æ€§èƒ½è¶‹åŠ¿ï¼Œæ­£æ•°è¡¨ç¤ºæ”¹å–„
    reliability_score: float = 0.0

class SmartToolSelectionEngine:
    """Smart Tool Engineæ™ºèƒ½é€‰æ‹©å±‚"""
    
    def __init__(self, config_path: str = "./smart_tool_config.json"):
        """åˆå§‹åŒ–æ™ºèƒ½é€‰æ‹©å¼•æ“"""
        self.config_path = Path(config_path)
        self.config = self._load_config()
        
        # æ ¸å¿ƒç»„ä»¶
        self.available_tools: Dict[str, MCPTool] = {}
        self.user_profiles: Dict[str, UserProfile] = {}
        self.tool_metrics: Dict[str, ToolPerformanceMetrics] = {}
        self.recommendation_cache: Dict[str, List[ToolRecommendation]] = {}
        
        # AIæ¨¡å‹å’ŒåµŒå…¥
        self.embeddings_cache: Dict[str, np.ndarray] = {}
        self.similarity_threshold = self.config.get("similarity_threshold", 0.7)
        
        # é€‰æ‹©å‚æ•°
        self.default_strategy = SelectionStrategy(self.config.get("default_strategy", "balanced"))
        self.max_recommendations = self.config.get("max_recommendations", 10)
        self.cache_ttl = self.config.get("cache_ttl", 3600)  # 1å°æ—¶
        
        # è¯„åˆ†æƒé‡
        self.scoring_weights = self.config.get("scoring_weights", {
            "capability": 0.3,
            "performance": 0.25,
            "cost": 0.15,
            "popularity": 0.1,
            "compatibility": 0.1,
            "user_preference": 0.1
        })
        
        # å­¦ä¹ å‚æ•°
        self.enable_learning = self.config.get("enable_learning", True)
        self.feedback_weight = self.config.get("feedback_weight", 0.2)
        
        # å­˜å‚¨è·¯å¾„
        self.data_dir = Path(self.config.get("data_dir", "./smart_tool_data"))
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½æŒä¹…åŒ–æ•°æ®
        self._load_persistent_data()
        
        logger.info("Smart Tool Engineæ™ºèƒ½é€‰æ‹©å±‚åˆå§‹åŒ–å®Œæˆ")
    
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®"""
        default_config = {
            "similarity_threshold": 0.7,
            "default_strategy": "balanced",
            "max_recommendations": 10,
            "cache_ttl": 3600,
            "scoring_weights": {
                "capability": 0.3,
                "performance": 0.25,
                "cost": 0.15,
                "popularity": 0.1,
                "compatibility": 0.1,
                "user_preference": 0.1
            },
            "enable_learning": True,
            "feedback_weight": 0.2,
            "data_dir": "./smart_tool_data",
            "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
            "enable_ensemble": True,
            "diversity_factor": 0.2
        }
        
        if self.config_path.exists():
            try:
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                default_config.update(user_config)
            except Exception as e:
                logger.warning(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
        
        return default_config
    
    def _load_persistent_data(self):
        """åŠ è½½æŒä¹…åŒ–æ•°æ®"""
        try:
            # åŠ è½½ç”¨æˆ·ç”»åƒ
            profiles_file = self.data_dir / "user_profiles.pkl"
            if profiles_file.exists():
                with open(profiles_file, 'rb') as f:
                    self.user_profiles = pickle.load(f)
            
            # åŠ è½½å·¥å…·æ€§èƒ½æŒ‡æ ‡
            metrics_file = self.data_dir / "tool_metrics.pkl"
            if metrics_file.exists():
                with open(metrics_file, 'rb') as f:
                    self.tool_metrics = pickle.load(f)
            
            # åŠ è½½åµŒå…¥ç¼“å­˜
            embeddings_file = self.data_dir / "embeddings_cache.pkl"
            if embeddings_file.exists():
                with open(embeddings_file, 'rb') as f:
                    self.embeddings_cache = pickle.load(f)
            
            logger.info("æŒä¹…åŒ–æ•°æ®åŠ è½½å®Œæˆ")
            
        except Exception as e:
            logger.warning(f"åŠ è½½æŒä¹…åŒ–æ•°æ®å¤±è´¥: {e}")
    
    def _save_persistent_data(self):
        """ä¿å­˜æŒä¹…åŒ–æ•°æ®"""
        try:
            # ä¿å­˜ç”¨æˆ·ç”»åƒ
            with open(self.data_dir / "user_profiles.pkl", 'wb') as f:
                pickle.dump(self.user_profiles, f)
            
            # ä¿å­˜å·¥å…·æ€§èƒ½æŒ‡æ ‡
            with open(self.data_dir / "tool_metrics.pkl", 'wb') as f:
                pickle.dump(self.tool_metrics, f)
            
            # ä¿å­˜åµŒå…¥ç¼“å­˜
            with open(self.data_dir / "embeddings_cache.pkl", 'wb') as f:
                pickle.dump(self.embeddings_cache, f)
            
            logger.debug("æŒä¹…åŒ–æ•°æ®ä¿å­˜å®Œæˆ")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æŒä¹…åŒ–æ•°æ®å¤±è´¥: {e}")
    
    async def register_tools(self, tools: List[MCPTool]):
        """æ³¨å†Œå·¥å…·"""
        for tool in tools:
            self.available_tools[tool.tool_id] = tool
            
            # åˆå§‹åŒ–å·¥å…·æ€§èƒ½æŒ‡æ ‡
            if tool.tool_id not in self.tool_metrics:
                self.tool_metrics[tool.tool_id] = ToolPerformanceMetrics(
                    tool_id=tool.tool_id,
                    reliability_score=0.8  # é»˜è®¤å¯é æ€§è¯„åˆ†
                )
        
        logger.info(f"æ³¨å†Œ {len(tools)} ä¸ªå·¥å…·")
    
    async def recommend_tools(self, task_requirement: TaskRequirement, 
                            user_id: str = None,
                            strategy: SelectionStrategy = None,
                            max_results: int = None) -> List[ToolRecommendation]:
        """æ¨èå·¥å…·"""
        if strategy is None:
            strategy = self.default_strategy
        
        if max_results is None:
            max_results = self.max_recommendations
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = self._generate_cache_key(task_requirement, user_id, strategy)
        if cache_key in self.recommendation_cache:
            cached_recommendations = self.recommendation_cache[cache_key]
            if self._is_cache_valid(cached_recommendations):
                logger.debug(f"ä½¿ç”¨ç¼“å­˜çš„æ¨èç»“æœ: {cache_key}")
                return cached_recommendations[:max_results]
        
        # è·å–ç”¨æˆ·ç”»åƒ
        user_profile = self._get_user_profile(user_id) if user_id else None
        
        # è®¡ç®—å·¥å…·è¯„åˆ†
        tool_scores = await self._calculate_tool_scores(task_requirement, user_profile, strategy)
        
        # ç”Ÿæˆæ¨è
        recommendations = await self._generate_recommendations(
            tool_scores, task_requirement, user_profile, strategy, max_results
        )
        
        # ç¼“å­˜ç»“æœ
        self.recommendation_cache[cache_key] = recommendations
        
        logger.info(f"ä¸ºä»»åŠ¡ç”Ÿæˆ {len(recommendations)} ä¸ªå·¥å…·æ¨è")
        return recommendations
    
    def _generate_cache_key(self, task_requirement: TaskRequirement, 
                          user_id: str, strategy: SelectionStrategy) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = f"{task_requirement.description}_{task_requirement.required_capabilities}_{user_id}_{strategy.value}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def _is_cache_valid(self, recommendations: List[ToolRecommendation]) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if not recommendations:
            return False
        
        # æ£€æŸ¥æ—¶é—´æˆ³
        first_rec = recommendations[0]
        if hasattr(first_rec, 'timestamp'):
            age = (datetime.now() - first_rec.timestamp).total_seconds()
            return age < self.cache_ttl
        
        return False
    
    def _get_user_profile(self, user_id: str) -> UserProfile:
        """è·å–ç”¨æˆ·ç”»åƒ"""
        if user_id not in self.user_profiles:
            self.user_profiles[user_id] = UserProfile(user_id=user_id)
        
        return self.user_profiles[user_id]
    
    async def _calculate_tool_scores(self, task_requirement: TaskRequirement,
                                   user_profile: Optional[UserProfile],
                                   strategy: SelectionStrategy) -> List[ToolScore]:
        """è®¡ç®—å·¥å…·è¯„åˆ†"""
        tool_scores = []
        
        for tool in self.available_tools.values():
            # åŸºç¡€èƒ½åŠ›åŒ¹é…è¯„åˆ†
            capability_score = await self._calculate_capability_score(tool, task_requirement)
            
            # æ€§èƒ½è¯„åˆ†
            performance_score = await self._calculate_performance_score(tool)
            
            # æˆæœ¬è¯„åˆ†
            cost_score = await self._calculate_cost_score(tool, task_requirement)
            
            # æµè¡Œåº¦è¯„åˆ†
            popularity_score = await self._calculate_popularity_score(tool)
            
            # å…¼å®¹æ€§è¯„åˆ†
            compatibility_score = await self._calculate_compatibility_score(tool, task_requirement)
            
            # ç”¨æˆ·åå¥½è¯„åˆ†
            user_preference_score = await self._calculate_user_preference_score(tool, user_profile)
            
            # ç»¼åˆè¯„åˆ†
            overall_score = self._calculate_overall_score(
                capability_score, performance_score, cost_score,
                popularity_score, compatibility_score, user_preference_score,
                strategy
            )
            
            # ç½®ä¿¡åº¦è®¡ç®—
            confidence = self._calculate_confidence(
                capability_score, performance_score, tool, task_requirement
            )
            
            # æ¨èåŸå› 
            reasons = self._determine_recommendation_reasons(
                capability_score, performance_score, cost_score,
                popularity_score, user_preference_score
            )
            
            tool_score = ToolScore(
                tool_id=tool.tool_id,
                overall_score=overall_score,
                capability_score=capability_score,
                performance_score=performance_score,
                cost_score=cost_score,
                popularity_score=popularity_score,
                compatibility_score=compatibility_score,
                user_preference_score=user_preference_score,
                confidence=confidence,
                reasons=reasons,
                details={
                    "tool_name": tool.name,
                    "category": tool.category.value,
                    "capabilities": [cap.name for cap in tool.capabilities]
                }
            )
            
            tool_scores.append(tool_score)
        
        # æŒ‰ç»¼åˆè¯„åˆ†æ’åº
        tool_scores.sort(key=lambda x: x.overall_score, reverse=True)
        
        return tool_scores
    
    async def _calculate_capability_score(self, tool: MCPTool, 
                                        task_requirement: TaskRequirement) -> float:
        """è®¡ç®—èƒ½åŠ›åŒ¹é…è¯„åˆ†"""
        if not task_requirement.required_capabilities:
            return 0.5  # é»˜è®¤è¯„åˆ†
        
        tool_capabilities = {cap.name for cap in tool.capabilities}
        required_capabilities = set(task_requirement.required_capabilities)
        
        if not required_capabilities:
            return 0.5
        
        # è®¡ç®—äº¤é›†æ¯”ä¾‹
        intersection = tool_capabilities.intersection(required_capabilities)
        capability_match_ratio = len(intersection) / len(required_capabilities)
        
        # è€ƒè™‘èƒ½åŠ›çš„ç½®ä¿¡åº¦
        confidence_bonus = 0.0
        for cap in tool.capabilities:
            if cap.name in required_capabilities:
                confidence_bonus += cap.confidence * 0.1
        
        # è¯­ä¹‰ç›¸ä¼¼æ€§è¯„åˆ†ï¼ˆå¦‚æœæœ‰åµŒå…¥æ¨¡å‹ï¼‰
        semantic_score = await self._calculate_semantic_similarity(tool, task_requirement)
        
        # ç»¼åˆè¯„åˆ†
        final_score = (
            capability_match_ratio * 0.6 +
            min(confidence_bonus, 0.3) +
            semantic_score * 0.1
        )
        
        return min(final_score, 1.0)
    
    async def _calculate_semantic_similarity(self, tool: MCPTool, 
                                           task_requirement: TaskRequirement) -> float:
        """è®¡ç®—è¯­ä¹‰ç›¸ä¼¼æ€§"""
        try:
            # è·å–å·¥å…·æè¿°åµŒå…¥
            tool_text = f"{tool.name} {tool.description} {' '.join([cap.name for cap in tool.capabilities])}"
            tool_embedding = await self._get_text_embedding(tool_text)
            
            # è·å–ä»»åŠ¡éœ€æ±‚åµŒå…¥
            task_text = f"{task_requirement.description} {' '.join(task_requirement.required_capabilities or [])}"
            task_embedding = await self._get_text_embedding(task_text)
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼æ€§
            if tool_embedding is not None and task_embedding is not None:
                similarity = np.dot(tool_embedding, task_embedding) / (
                    np.linalg.norm(tool_embedding) * np.linalg.norm(task_embedding)
                )
                return max(0.0, similarity)
            
        except Exception as e:
            logger.debug(f"è®¡ç®—è¯­ä¹‰ç›¸ä¼¼æ€§å¤±è´¥: {e}")
        
        return 0.0
    
    async def _get_text_embedding(self, text: str) -> Optional[np.ndarray]:
        """è·å–æ–‡æœ¬åµŒå…¥"""
        # ç®€åŒ–å®ç°ï¼šä½¿ç”¨ç¼“å­˜çš„éšæœºåµŒå…¥
        text_hash = hashlib.md5(text.encode()).hexdigest()
        
        if text_hash not in self.embeddings_cache:
            # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥è°ƒç”¨çœŸå®çš„åµŒå…¥æ¨¡å‹
            # ä¾‹å¦‚ï¼šsentence-transformers, OpenAI embeddingsç­‰
            np.random.seed(int(text_hash[:8], 16))
            embedding = np.random.normal(0, 1, 384)  # 384ç»´åµŒå…¥
            embedding = embedding / np.linalg.norm(embedding)  # å½’ä¸€åŒ–
            self.embeddings_cache[text_hash] = embedding
        
        return self.embeddings_cache[text_hash]
    
    async def _calculate_performance_score(self, tool: MCPTool) -> float:
        """è®¡ç®—æ€§èƒ½è¯„åˆ†"""
        metrics = self.tool_metrics.get(tool.tool_id)
        if not metrics:
            return 0.6  # é»˜è®¤è¯„åˆ†
        
        # æˆåŠŸç‡æƒé‡
        success_score = metrics.success_rate * 0.4
        
        # å¯é æ€§æƒé‡
        reliability_score = metrics.reliability_score * 0.3
        
        # ç”¨æˆ·æ»¡æ„åº¦æƒé‡
        satisfaction_score = metrics.user_satisfaction * 0.2
        
        # æ€§èƒ½è¶‹åŠ¿æƒé‡
        trend_score = min(max(metrics.performance_trend + 0.5, 0), 1) * 0.1
        
        return success_score + reliability_score + satisfaction_score + trend_score
    
    async def _calculate_cost_score(self, tool: MCPTool, 
                                  task_requirement: TaskRequirement) -> float:
        """è®¡ç®—æˆæœ¬è¯„åˆ†"""
        # ç®€åŒ–çš„æˆæœ¬æ¨¡å‹
        base_cost = 0.5  # åŸºç¡€æˆæœ¬
        
        # æ ¹æ®å·¥å…·å¤æ‚åº¦è°ƒæ•´æˆæœ¬
        complexity_factor = len(tool.capabilities) * 0.05
        
        # æ ¹æ®ä»»åŠ¡å¤æ‚åº¦è°ƒæ•´æˆæœ¬
        task_complexity = task_requirement.complexity_level or "medium"
        complexity_multiplier = {
            "low": 0.8,
            "medium": 1.0,
            "high": 1.3
        }.get(task_complexity, 1.0)
        
        estimated_cost = (base_cost + complexity_factor) * complexity_multiplier
        
        # æˆæœ¬è¯„åˆ†ï¼ˆæˆæœ¬è¶Šä½è¯„åˆ†è¶Šé«˜ï¼‰
        cost_score = max(0, 1.0 - estimated_cost)
        
        return cost_score
    
    async def _calculate_popularity_score(self, tool: MCPTool) -> float:
        """è®¡ç®—æµè¡Œåº¦è¯„åˆ†"""
        metrics = self.tool_metrics.get(tool.tool_id)
        if not metrics:
            return 0.5
        
        # ä½¿ç”¨é¢‘ç‡å½’ä¸€åŒ–
        max_usage = max([m.usage_frequency for m in self.tool_metrics.values()], default=1)
        usage_score = metrics.usage_frequency / max_usage if max_usage > 0 else 0
        
        # æœ€è¿‘ä½¿ç”¨æ—¶é—´
        recency_score = 0.5
        if metrics.last_used:
            days_since_use = (datetime.now() - metrics.last_used).days
            recency_score = max(0, 1.0 - days_since_use / 30)  # 30å¤©å†…çš„ä½¿ç”¨
        
        return (usage_score * 0.7 + recency_score * 0.3)
    
    async def _calculate_compatibility_score(self, tool: MCPTool, 
                                           task_requirement: TaskRequirement) -> float:
        """è®¡ç®—å…¼å®¹æ€§è¯„åˆ†"""
        score = 0.8  # åŸºç¡€å…¼å®¹æ€§è¯„åˆ†
        
        # æ£€æŸ¥ç¯å¢ƒè¦æ±‚
        if hasattr(task_requirement, 'environment_constraints'):
            constraints = task_requirement.environment_constraints or {}
            
            # æ“ä½œç³»ç»Ÿå…¼å®¹æ€§
            if 'os' in constraints:
                required_os = constraints['os']
                if hasattr(tool, 'supported_os'):
                    if required_os in tool.supported_os:
                        score += 0.1
                    else:
                        score -= 0.2
            
            # ä¾èµ–å…¼å®¹æ€§
            if 'dependencies' in constraints:
                # ç®€åŒ–çš„ä¾èµ–æ£€æŸ¥
                score += 0.1
        
        return min(max(score, 0), 1.0)
    
    async def _calculate_user_preference_score(self, tool: MCPTool, 
                                             user_profile: Optional[UserProfile]) -> float:
        """è®¡ç®—ç”¨æˆ·åå¥½è¯„åˆ†"""
        if not user_profile:
            return 0.5
        
        score = 0.5
        
        # æ”¶è—å·¥å…·
        if tool.tool_id in user_profile.favorite_tools:
            score += 0.3
        
        # é¿å…çš„å·¥å…·
        if tool.tool_id in user_profile.avoided_tools:
            score -= 0.4
        
        # ç±»åˆ«åå¥½
        category_pref = user_profile.preferences.get(tool.category.value, 0.5)
        score += (category_pref - 0.5) * 0.2
        
        # ä½¿ç”¨å†å²
        for usage in user_profile.usage_history[-10:]:  # æœ€è¿‘10æ¬¡ä½¿ç”¨
            if usage.get('tool_id') == tool.tool_id:
                rating = usage.get('rating', 0.5)
                score += (rating - 0.5) * 0.1
        
        return min(max(score, 0), 1.0)
    
    def _calculate_overall_score(self, capability_score: float, performance_score: float,
                               cost_score: float, popularity_score: float,
                               compatibility_score: float, user_preference_score: float,
                               strategy: SelectionStrategy) -> float:
        """è®¡ç®—ç»¼åˆè¯„åˆ†"""
        weights = self.scoring_weights.copy()
        
        # æ ¹æ®ç­–ç•¥è°ƒæ•´æƒé‡
        if strategy == SelectionStrategy.PERFORMANCE_FIRST:
            weights["performance"] *= 2.0
            weights["capability"] *= 1.5
        elif strategy == SelectionStrategy.COST_OPTIMIZED:
            weights["cost"] *= 2.5
            weights["performance"] *= 0.8
        elif strategy == SelectionStrategy.USER_PREFERENCE:
            weights["user_preference"] *= 3.0
            weights["popularity"] *= 1.5
        elif strategy == SelectionStrategy.BEST_MATCH:
            weights["capability"] *= 2.0
            weights["compatibility"] *= 1.5
        
        # å½’ä¸€åŒ–æƒé‡
        total_weight = sum(weights.values())
        for key in weights:
            weights[key] /= total_weight
        
        # è®¡ç®—åŠ æƒè¯„åˆ†
        overall_score = (
            capability_score * weights["capability"] +
            performance_score * weights["performance"] +
            cost_score * weights["cost"] +
            popularity_score * weights["popularity"] +
            compatibility_score * weights["compatibility"] +
            user_preference_score * weights["user_preference"]
        )
        
        return overall_score
    
    def _calculate_confidence(self, capability_score: float, performance_score: float,
                            tool: MCPTool, task_requirement: TaskRequirement) -> float:
        """è®¡ç®—ç½®ä¿¡åº¦"""
        # åŸºäºèƒ½åŠ›åŒ¹é…å’Œæ€§èƒ½çš„ç½®ä¿¡åº¦
        base_confidence = (capability_score + performance_score) / 2
        
        # å·¥å…·æˆç†Ÿåº¦è°ƒæ•´
        maturity_bonus = 0.0
        if hasattr(tool, 'version'):
            try:
                version_parts = tool.version.split('.')
                major_version = int(version_parts[0])
                if major_version >= 1:
                    maturity_bonus = 0.1
            except:
                pass
        
        # æ–‡æ¡£å®Œæ•´æ€§è°ƒæ•´
        doc_bonus = 0.0
        if tool.documentation_url or tool.examples:
            doc_bonus = 0.05
        
        confidence = base_confidence + maturity_bonus + doc_bonus
        return min(confidence, 1.0)
    
    def _determine_recommendation_reasons(self, capability_score: float, performance_score: float,
                                        cost_score: float, popularity_score: float,
                                        user_preference_score: float) -> List[RecommendationReason]:
        """ç¡®å®šæ¨èåŸå› """
        reasons = []
        
        if capability_score > 0.8:
            reasons.append(RecommendationReason.CAPABILITY_MATCH)
        
        if performance_score > 0.8:
            reasons.append(RecommendationReason.PERFORMANCE_SCORE)
        
        if cost_score > 0.8:
            reasons.append(RecommendationReason.COST_EFFICIENCY)
        
        if popularity_score > 0.7:
            reasons.append(RecommendationReason.POPULARITY)
        
        if user_preference_score > 0.7:
            reasons.append(RecommendationReason.USER_HISTORY)
        
        return reasons
    
    async def _generate_recommendations(self, tool_scores: List[ToolScore],
                                      task_requirement: TaskRequirement,
                                      user_profile: Optional[UserProfile],
                                      strategy: SelectionStrategy,
                                      max_results: int) -> List[ToolRecommendation]:
        """ç”Ÿæˆæ¨è"""
        recommendations = []
        
        # åº”ç”¨å¤šæ ·æ€§è¿‡æ»¤
        if self.config.get("enable_diversity", True):
            tool_scores = self._apply_diversity_filter(tool_scores)
        
        for i, score in enumerate(tool_scores[:max_results]):
            tool = self.available_tools[score.tool_id]
            
            recommendation = ToolRecommendation(
                recommendation_id=f"rec_{uuid.uuid4().hex[:8]}",
                tool_id=tool.tool_id,
                task_id=task_requirement.task_id,
                user_id=user_profile.user_id if user_profile else None,
                score=score.overall_score,
                confidence=score.confidence,
                rank=i + 1,
                reasons=score.reasons,
                explanation=self._generate_explanation(tool, score, task_requirement),
                estimated_cost=self._estimate_tool_cost(tool, task_requirement),
                estimated_time=self._estimate_execution_time(tool, task_requirement),
                alternatives=[],  # å¯ä»¥æ·»åŠ æ›¿ä»£å·¥å…·
                metadata={
                    "strategy": strategy.value,
                    "score_breakdown": {
                        "capability": score.capability_score,
                        "performance": score.performance_score,
                        "cost": score.cost_score,
                        "popularity": score.popularity_score,
                        "compatibility": score.compatibility_score,
                        "user_preference": score.user_preference_score
                    },
                    "tool_details": score.details
                },
                timestamp=datetime.now()
            )
            
            recommendations.append(recommendation)
        
        return recommendations
    
    def _apply_diversity_filter(self, tool_scores: List[ToolScore]) -> List[ToolScore]:
        """åº”ç”¨å¤šæ ·æ€§è¿‡æ»¤"""
        if not tool_scores:
            return tool_scores
        
        diversity_factor = self.config.get("diversity_factor", 0.2)
        filtered_scores = [tool_scores[0]]  # ä¿ç•™æœ€é«˜åˆ†çš„å·¥å…·
        
        for score in tool_scores[1:]:
            tool = self.available_tools[score.tool_id]
            
            # æ£€æŸ¥ä¸å·²é€‰å·¥å…·çš„å¤šæ ·æ€§
            is_diverse = True
            for selected_score in filtered_scores:
                selected_tool = self.available_tools[selected_score.tool_id]
                
                # æ£€æŸ¥ç±»åˆ«å¤šæ ·æ€§
                if tool.category == selected_tool.category:
                    # å¦‚æœåŒç±»åˆ«ï¼Œéœ€è¦æ›´é«˜çš„è¯„åˆ†æ‰èƒ½å…¥é€‰
                    required_score = selected_score.overall_score + diversity_factor
                    if score.overall_score < required_score:
                        is_diverse = False
                        break
            
            if is_diverse:
                filtered_scores.append(score)
        
        return filtered_scores
    
    def _generate_explanation(self, tool: MCPTool, score: ToolScore, 
                            task_requirement: TaskRequirement) -> str:
        """ç”Ÿæˆæ¨èè§£é‡Š"""
        explanations = []
        
        # åŸºäºæ¨èåŸå› ç”Ÿæˆè§£é‡Š
        if RecommendationReason.CAPABILITY_MATCH in score.reasons:
            explanations.append(f"è¯¥å·¥å…·çš„èƒ½åŠ›ä¸ä»»åŠ¡éœ€æ±‚é«˜åº¦åŒ¹é…ï¼ˆåŒ¹é…åº¦ï¼š{score.capability_score:.1%}ï¼‰")
        
        if RecommendationReason.PERFORMANCE_SCORE in score.reasons:
            explanations.append(f"è¯¥å·¥å…·å…·æœ‰ä¼˜ç§€çš„æ€§èƒ½è¡¨ç°ï¼ˆæ€§èƒ½è¯„åˆ†ï¼š{score.performance_score:.1%}ï¼‰")
        
        if RecommendationReason.COST_EFFICIENCY in score.reasons:
            explanations.append(f"è¯¥å·¥å…·å…·æœ‰è‰¯å¥½çš„æˆæœ¬æ•ˆç›Šï¼ˆæˆæœ¬è¯„åˆ†ï¼š{score.cost_score:.1%}ï¼‰")
        
        if RecommendationReason.POPULARITY in score.reasons:
            explanations.append(f"è¯¥å·¥å…·åœ¨ç¤¾åŒºä¸­å¹¿å—æ¬¢è¿ï¼ˆæµè¡Œåº¦ï¼š{score.popularity_score:.1%}ï¼‰")
        
        if RecommendationReason.USER_HISTORY in score.reasons:
            explanations.append(f"åŸºäºæ‚¨çš„ä½¿ç”¨å†å²ï¼Œæ‚¨å¯èƒ½ä¼šå–œæ¬¢è¿™ä¸ªå·¥å…·")
        
        # æ·»åŠ å·¥å…·ç‰¹è‰²
        if tool.capabilities:
            top_capabilities = [cap.name for cap in tool.capabilities[:3]]
            explanations.append(f"ä¸»è¦èƒ½åŠ›ï¼š{', '.join(top_capabilities)}")
        
        return "ï¼›".join(explanations) if explanations else f"æ¨èä½¿ç”¨ {tool.name} æ¥å®Œæˆæ­¤ä»»åŠ¡"
    
    def _estimate_tool_cost(self, tool: MCPTool, task_requirement: TaskRequirement) -> float:
        """ä¼°ç®—å·¥å…·æˆæœ¬"""
        # ç®€åŒ–çš„æˆæœ¬ä¼°ç®—æ¨¡å‹
        base_cost = 0.1  # åŸºç¡€æˆæœ¬
        
        # æ ¹æ®å·¥å…·å¤æ‚åº¦è°ƒæ•´
        complexity_cost = len(tool.capabilities) * 0.02
        
        # æ ¹æ®ä»»åŠ¡å¤æ‚åº¦è°ƒæ•´
        task_complexity = task_requirement.complexity_level or "medium"
        complexity_multiplier = {
            "low": 0.8,
            "medium": 1.0,
            "high": 1.5
        }.get(task_complexity, 1.0)
        
        return (base_cost + complexity_cost) * complexity_multiplier
    
    def _estimate_execution_time(self, tool: MCPTool, task_requirement: TaskRequirement) -> float:
        """ä¼°ç®—æ‰§è¡Œæ—¶é—´ï¼ˆç§’ï¼‰"""
        metrics = self.tool_metrics.get(tool.tool_id)
        if metrics and metrics.average_execution_time > 0:
            return metrics.average_execution_time
        
        # åŸºäºå·¥å…·å¤æ‚åº¦çš„ä¼°ç®—
        base_time = 5.0  # åŸºç¡€æ—¶é—´ï¼ˆç§’ï¼‰
        complexity_time = len(tool.capabilities) * 2.0
        
        # æ ¹æ®ä»»åŠ¡å¤æ‚åº¦è°ƒæ•´
        task_complexity = task_requirement.complexity_level or "medium"
        complexity_multiplier = {
            "low": 0.5,
            "medium": 1.0,
            "high": 2.0
        }.get(task_complexity, 1.0)
        
        return (base_time + complexity_time) * complexity_multiplier
    
    async def provide_feedback(self, recommendation_id: str, user_id: str, 
                             feedback: Dict[str, Any]):
        """æä¾›åé¦ˆ"""
        if not self.enable_learning:
            return
        
        try:
            # æ›´æ–°ç”¨æˆ·ç”»åƒ
            user_profile = self._get_user_profile(user_id)
            
            # è®°å½•ä½¿ç”¨å†å²
            usage_record = {
                "recommendation_id": recommendation_id,
                "timestamp": datetime.now().isoformat(),
                "rating": feedback.get("rating", 0.5),
                "success": feedback.get("success", True),
                "execution_time": feedback.get("execution_time"),
                "comments": feedback.get("comments", "")
            }
            
            user_profile.usage_history.append(usage_record)
            
            # é™åˆ¶å†å²è®°å½•é•¿åº¦
            if len(user_profile.usage_history) > 100:
                user_profile.usage_history = user_profile.usage_history[-100:]
            
            # æ›´æ–°å·¥å…·æ€§èƒ½æŒ‡æ ‡
            tool_id = feedback.get("tool_id")
            if tool_id and tool_id in self.tool_metrics:
                metrics = self.tool_metrics[tool_id]
                
                # æ›´æ–°æˆåŠŸç‡
                if "success" in feedback:
                    old_rate = metrics.success_rate
                    new_success = 1.0 if feedback["success"] else 0.0
                    metrics.success_rate = old_rate * 0.9 + new_success * 0.1
                
                # æ›´æ–°ç”¨æˆ·æ»¡æ„åº¦
                if "rating" in feedback:
                    old_satisfaction = metrics.user_satisfaction
                    new_rating = feedback["rating"]
                    metrics.user_satisfaction = old_satisfaction * 0.9 + new_rating * 0.1
                
                # æ›´æ–°æ‰§è¡Œæ—¶é—´
                if "execution_time" in feedback:
                    old_time = metrics.average_execution_time
                    new_time = feedback["execution_time"]
                    if old_time > 0:
                        metrics.average_execution_time = old_time * 0.9 + new_time * 0.1
                    else:
                        metrics.average_execution_time = new_time
                
                # æ›´æ–°ä½¿ç”¨é¢‘ç‡
                metrics.usage_frequency += 1
                metrics.last_used = datetime.now()
            
            # å­¦ä¹ ç”¨æˆ·åå¥½
            await self._update_user_preferences(user_profile, feedback)
            
            # ä¿å­˜æ•°æ®
            self._save_persistent_data()
            
            logger.info(f"å¤„ç†ç”¨æˆ· {user_id} çš„åé¦ˆï¼š{recommendation_id}")
            
        except Exception as e:
            logger.error(f"å¤„ç†åé¦ˆå¤±è´¥: {e}")
    
    async def _update_user_preferences(self, user_profile: UserProfile, 
                                     feedback: Dict[str, Any]):
        """æ›´æ–°ç”¨æˆ·åå¥½"""
        tool_id = feedback.get("tool_id")
        if not tool_id or tool_id not in self.available_tools:
            return
        
        tool = self.available_tools[tool_id]
        rating = feedback.get("rating", 0.5)
        learning_rate = user_profile.learning_rate
        
        # æ›´æ–°ç±»åˆ«åå¥½
        category = tool.category.value
        old_pref = user_profile.preferences.get(category, 0.5)
        new_pref = old_pref + learning_rate * (rating - old_pref)
        user_profile.preferences[category] = max(0, min(1, new_pref))
        
        # æ›´æ–°æ”¶è—å’Œé¿å…åˆ—è¡¨
        if rating >= 0.8:
            user_profile.favorite_tools.add(tool_id)
            user_profile.avoided_tools.discard(tool_id)
        elif rating <= 0.2:
            user_profile.avoided_tools.add(tool_id)
            user_profile.favorite_tools.discard(tool_id)
        
        user_profile.last_updated = datetime.now()
    
    async def get_recommendation_analytics(self, user_id: str = None, 
                                         days: int = 30) -> Dict[str, Any]:
        """è·å–æ¨èåˆ†æ"""
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days)
        
        analytics = {
            "period": {"start": start_date.isoformat(), "end": end_date.isoformat()},
            "total_recommendations": 0,
            "successful_recommendations": 0,
            "average_rating": 0.0,
            "top_tools": [],
            "category_distribution": {},
            "strategy_performance": {},
            "user_satisfaction_trend": []
        }
        
        # åˆ†æç”¨æˆ·å†å²ï¼ˆå¦‚æœæŒ‡å®šç”¨æˆ·ï¼‰
        if user_id and user_id in self.user_profiles:
            user_profile = self.user_profiles[user_id]
            recent_usage = [
                usage for usage in user_profile.usage_history
                if datetime.fromisoformat(usage["timestamp"]) >= start_date
            ]
            
            analytics["total_recommendations"] = len(recent_usage)
            
            if recent_usage:
                successful_count = sum(1 for usage in recent_usage if usage.get("success", True))
                analytics["successful_recommendations"] = successful_count
                
                ratings = [usage.get("rating", 0.5) for usage in recent_usage]
                analytics["average_rating"] = sum(ratings) / len(ratings)
        
        # åˆ†æå·¥å…·æ€§èƒ½
        tool_performance = []
        for tool_id, metrics in self.tool_metrics.items():
            if tool_id in self.available_tools:
                tool = self.available_tools[tool_id]
                tool_performance.append({
                    "tool_id": tool_id,
                    "tool_name": tool.name,
                    "usage_frequency": metrics.usage_frequency,
                    "success_rate": metrics.success_rate,
                    "user_satisfaction": metrics.user_satisfaction
                })
        
        # æ’åºå¹¶è·å–å‰10ä¸ªå·¥å…·
        tool_performance.sort(key=lambda x: x["usage_frequency"], reverse=True)
        analytics["top_tools"] = tool_performance[:10]
        
        # ç±»åˆ«åˆ†å¸ƒ
        for tool in self.available_tools.values():
            category = tool.category.value
            analytics["category_distribution"][category] = analytics["category_distribution"].get(category, 0) + 1
        
        return analytics
    
    async def optimize_selection_parameters(self):
        """ä¼˜åŒ–é€‰æ‹©å‚æ•°"""
        if not self.enable_learning:
            return
        
        try:
            # åˆ†æå†å²åé¦ˆæ•°æ®
            all_feedback = []
            for user_profile in self.user_profiles.values():
                all_feedback.extend(user_profile.usage_history)
            
            if len(all_feedback) < 10:  # æ•°æ®ä¸è¶³
                return
            
            # è®¡ç®—å½“å‰å‚æ•°çš„æ€§èƒ½
            current_performance = sum(feedback.get("rating", 0.5) for feedback in all_feedback) / len(all_feedback)
            
            # ç®€åŒ–çš„å‚æ•°ä¼˜åŒ–ï¼šè°ƒæ•´è¯„åˆ†æƒé‡
            best_weights = self.scoring_weights.copy()
            best_performance = current_performance
            
            # å°è¯•ä¸åŒçš„æƒé‡ç»„åˆ
            weight_adjustments = [0.9, 1.1]  # Â±10%è°ƒæ•´
            
            for weight_name in self.scoring_weights:
                for adjustment in weight_adjustments:
                    test_weights = self.scoring_weights.copy()
                    test_weights[weight_name] *= adjustment
                    
                    # å½’ä¸€åŒ–æƒé‡
                    total_weight = sum(test_weights.values())
                    for key in test_weights:
                        test_weights[key] /= total_weight
                    
                    # è¯„ä¼°æ€§èƒ½ï¼ˆç®€åŒ–å®ç°ï¼‰
                    estimated_performance = current_performance + (adjustment - 1.0) * 0.1
                    
                    if estimated_performance > best_performance:
                        best_performance = estimated_performance
                        best_weights = test_weights
            
            # æ›´æ–°æƒé‡
            if best_performance > current_performance:
                self.scoring_weights = best_weights
                logger.info(f"ä¼˜åŒ–é€‰æ‹©å‚æ•°ï¼Œæ€§èƒ½æå‡ï¼š{best_performance - current_performance:.3f}")
            
        except Exception as e:
            logger.error(f"ä¼˜åŒ–é€‰æ‹©å‚æ•°å¤±è´¥: {e}")

# å·¥å‚å‡½æ•°
def get_smart_tool_selection_engine(config_path: str = "./smart_tool_config.json") -> SmartToolSelectionEngine:
    """è·å–Smart Tool Engineæ™ºèƒ½é€‰æ‹©å±‚å®ä¾‹"""
    return SmartToolSelectionEngine(config_path)

# æµ‹è¯•å’Œæ¼”ç¤º
if __name__ == "__main__":
    async def test_selection_engine():
        """æµ‹è¯•æ™ºèƒ½é€‰æ‹©å¼•æ“"""
        engine = get_smart_tool_selection_engine()
        
        # åˆ›å»ºæµ‹è¯•å·¥å…·
        test_tools = [
            MCPTool(
                tool_id="file_manager",
                name="File Manager",
                description="Manage files and directories",
                capabilities=[
                    ToolCapability(name="file_operations", confidence=0.9),
                    ToolCapability(name="directory_management", confidence=0.8)
                ],
                category=ToolCategory.UTILITY
            ),
            MCPTool(
                tool_id="web_scraper",
                name="Web Scraper",
                description="Extract data from websites",
                capabilities=[
                    ToolCapability(name="web_scraping", confidence=0.95),
                    ToolCapability(name="data_extraction", confidence=0.8)
                ],
                category=ToolCategory.WEB_AUTOMATION
            )
        ]
        
        # æ³¨å†Œå·¥å…·
        await engine.register_tools(test_tools)
        
        # åˆ›å»ºä»»åŠ¡éœ€æ±‚
        task_requirement = TaskRequirement(
            task_id="test_task",
            description="Extract data from a website and save to file",
            required_capabilities=["web_scraping", "file_operations"],
            complexity_level="medium"
        )
        
        # è·å–æ¨è
        recommendations = await engine.recommend_tools(
            task_requirement=task_requirement,
            user_id="test_user",
            strategy=SelectionStrategy.BALANCED
        )
        
        print(f"ğŸ¯ è·å¾— {len(recommendations)} ä¸ªå·¥å…·æ¨è:")
        for rec in recommendations:
            tool = engine.available_tools[rec.tool_id]
            print(f"  {rec.rank}. {tool.name} (è¯„åˆ†: {rec.score:.3f})")
            print(f"     {rec.explanation}")
            print(f"     é¢„ä¼°æˆæœ¬: ${rec.estimated_cost:.2f}, é¢„ä¼°æ—¶é—´: {rec.estimated_time:.1f}ç§’")
            print()
        
        # æä¾›åé¦ˆ
        if recommendations:
            await engine.provide_feedback(
                recommendation_id=recommendations[0].recommendation_id,
                user_id="test_user",
                feedback={
                    "tool_id": recommendations[0].tool_id,
                    "rating": 0.9,
                    "success": True,
                    "execution_time": 15.5,
                    "comments": "å·¥å…·è¿è¡Œè‰¯å¥½"
                }
            )
            print("âœ… åé¦ˆå·²æäº¤")
        
        # è·å–åˆ†æ
        analytics = await engine.get_recommendation_analytics("test_user")
        print(f"ğŸ“Š æ¨èåˆ†æ:")
        print(f"  æ€»æ¨èæ•°: {analytics['total_recommendations']}")
        print(f"  æˆåŠŸæ¨èæ•°: {analytics['successful_recommendations']}")
        print(f"  å¹³å‡è¯„åˆ†: {analytics['average_rating']:.2f}")
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_selection_engine())

