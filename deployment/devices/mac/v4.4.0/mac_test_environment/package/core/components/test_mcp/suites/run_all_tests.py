#!/usr/bin/env python3
"""
PowerAutomation 4.0 ä¸»æµ‹è¯•è¿è¡Œå™¨
æŒ‰ç…§AICore0624çš„æµ‹è¯•æ–¹æ³•è¿›è¡Œä¸¥è°¨æµ‹è¯•
"""

import asyncio
import sys
import os
import time
import json
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional
import unittest
import pytest

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥æµ‹è¯•æ¨¡å—
from tests.unit_tests.test_core_components import TestCoreComponents
from tests.unit_tests.test_smart_router_mcp import TestSmartRouterMCP
from tests.unit_tests.test_mcp_coordinator import TestMCPCoordinator
from tests.unit_tests.test_agent_squad import TestAgentSquad
from tests.integration_tests.test_mcp_integration import TestMCPIntegration
from tests.integration_tests.test_agent_integration import TestAgentIntegration
from tests.api_tests.test_api_endpoints import TestAPIEndpoints
from tests.performance_tests.test_performance import TestPerformance

class PowerAutomationTestRunner:
    """PowerAutomation 4.0 æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self):
        self.logger = self._setup_logging()
        self.test_results = {
            "start_time": None,
            "end_time": None,
            "total_duration": 0,
            "test_suites": {},
            "summary": {
                "total_tests": 0,
                "passed": 0,
                "failed": 0,
                "skipped": 0,
                "errors": 0
            },
            "coverage": {},
            "performance_metrics": {}
        }
        
    def _setup_logging(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(f'tests/logs/test_run_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        return logging.getLogger(__name__)
    
    async def run_all_tests(self, test_types: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        è¿è¡Œæ‰€æœ‰æµ‹è¯•
        
        Args:
            test_types: è¦è¿è¡Œçš„æµ‹è¯•ç±»å‹åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºè¿è¡Œæ‰€æœ‰æµ‹è¯•
            
        Returns:
            Dict[str, Any]: æµ‹è¯•ç»“æœ
        """
        self.test_results["start_time"] = datetime.now()
        start_time = time.time()
        
        self.logger.info("ğŸš€ PowerAutomation 4.0 æµ‹è¯•å¼€å§‹")
        self.logger.info("=" * 80)
        
        # é»˜è®¤è¿è¡Œæ‰€æœ‰æµ‹è¯•ç±»å‹
        if test_types is None:
            test_types = ["unit", "integration", "api", "performance"]
        
        try:
            # 1. å•å…ƒæµ‹è¯•
            if "unit" in test_types:
                await self._run_unit_tests()
            
            # 2. é›†æˆæµ‹è¯•
            if "integration" in test_types:
                await self._run_integration_tests()
            
            # 3. APIæµ‹è¯•
            if "api" in test_types:
                await self._run_api_tests()
            
            # 4. æ€§èƒ½æµ‹è¯•
            if "performance" in test_types:
                await self._run_performance_tests()
            
            # 5. ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
            await self._generate_test_report()
            
            # 6. è®¡ç®—æ€»ä½“ç»“æœ
            self._calculate_summary()
            
        except Exception as e:
            self.logger.error(f"æµ‹è¯•è¿è¡Œå¤±è´¥: {e}")
            self.test_results["summary"]["errors"] += 1
        
        finally:
            self.test_results["end_time"] = datetime.now()
            self.test_results["total_duration"] = time.time() - start_time
            
            # è¾“å‡ºæœ€ç»ˆç»“æœ
            await self._print_final_results()
        
        return self.test_results
    
    async def _run_unit_tests(self):
        """è¿è¡Œå•å…ƒæµ‹è¯•"""
        self.logger.info("ğŸ“‹ å¼€å§‹å•å…ƒæµ‹è¯•")
        self.logger.info("-" * 40)
        
        unit_test_suites = [
            ("æ ¸å¿ƒç»„ä»¶æµ‹è¯•", TestCoreComponents),
            ("æ™ºæ…§è·¯ç”±MCPæµ‹è¯•", TestSmartRouterMCP),
            ("MCPåè°ƒå™¨æµ‹è¯•", TestMCPCoordinator),
            ("æ™ºèƒ½ä½“ç³»ç»Ÿæµ‹è¯•", TestAgentSquad)
        ]
        
        unit_results = {
            "total_tests": 0,
            "passed": 0,
            "failed": 0,
            "skipped": 0,
            "test_cases": []
        }
        
        for suite_name, test_class in unit_test_suites:
            self.logger.info(f"ğŸ§ª è¿è¡Œ {suite_name}")
            
            try:
                # åˆ›å»ºæµ‹è¯•å¥—ä»¶
                suite = unittest.TestLoader().loadTestsFromTestCase(test_class)
                
                # è¿è¡Œæµ‹è¯•
                runner = unittest.TextTestRunner(verbosity=2, stream=open(os.devnull, 'w'))
                result = runner.run(suite)
                
                # è®°å½•ç»“æœ
                test_case_result = {
                    "suite_name": suite_name,
                    "tests_run": result.testsRun,
                    "failures": len(result.failures),
                    "errors": len(result.errors),
                    "skipped": len(result.skipped) if hasattr(result, 'skipped') else 0,
                    "success": result.wasSuccessful()
                }
                
                unit_results["test_cases"].append(test_case_result)
                unit_results["total_tests"] += result.testsRun
                unit_results["failed"] += len(result.failures) + len(result.errors)
                unit_results["skipped"] += len(result.skipped) if hasattr(result, 'skipped') else 0
                unit_results["passed"] += result.testsRun - len(result.failures) - len(result.errors)
                
                status = "âœ… é€šè¿‡" if result.wasSuccessful() else "âŒ å¤±è´¥"
                self.logger.info(f"   {status} - {result.testsRun} ä¸ªæµ‹è¯•")
                
            except Exception as e:
                self.logger.error(f"   âŒ æµ‹è¯•å¥—ä»¶æ‰§è¡Œå¤±è´¥: {e}")
                unit_results["failed"] += 1
        
        self.test_results["test_suites"]["unit_tests"] = unit_results
        self.logger.info(f"ğŸ“‹ å•å…ƒæµ‹è¯•å®Œæˆ: {unit_results['passed']}/{unit_results['total_tests']} é€šè¿‡")
    
    async def _run_integration_tests(self):
        """è¿è¡Œé›†æˆæµ‹è¯•"""
        self.logger.info("ğŸ”— å¼€å§‹é›†æˆæµ‹è¯•")
        self.logger.info("-" * 40)
        
        integration_results = {
            "total_tests": 0,
            "passed": 0,
            "failed": 0,
            "test_cases": []
        }
        
        # MCPé›†æˆæµ‹è¯•
        try:
            self.logger.info("ğŸ§© MCPç»„ä»¶é›†æˆæµ‹è¯•")
            mcp_test = TestMCPIntegration()
            await mcp_test.setUp()
            
            # è¿è¡ŒMCPé›†æˆæµ‹è¯•
            mcp_result = await mcp_test.test_mcp_communication()
            integration_results["test_cases"].append({
                "name": "MCPé€šä¿¡é›†æˆ",
                "result": mcp_result,
                "success": mcp_result.get("status") == "success"
            })
            
            # æ™ºèƒ½ä½“é›†æˆæµ‹è¯•
            self.logger.info("ğŸ¤– æ™ºèƒ½ä½“ç³»ç»Ÿé›†æˆæµ‹è¯•")
            agent_test = TestAgentIntegration()
            await agent_test.setUp()
            
            agent_result = await agent_test.test_agent_collaboration()
            integration_results["test_cases"].append({
                "name": "æ™ºèƒ½ä½“åä½œé›†æˆ",
                "result": agent_result,
                "success": agent_result.get("status") == "success"
            })
            
            # ç«¯åˆ°ç«¯å·¥ä½œæµæµ‹è¯•
            self.logger.info("ğŸ”„ ç«¯åˆ°ç«¯å·¥ä½œæµæµ‹è¯•")
            workflow_result = await self._test_end_to_end_workflow()
            integration_results["test_cases"].append({
                "name": "ç«¯åˆ°ç«¯å·¥ä½œæµ",
                "result": workflow_result,
                "success": workflow_result.get("status") == "success"
            })
            
        except Exception as e:
            self.logger.error(f"é›†æˆæµ‹è¯•å¤±è´¥: {e}")
            integration_results["failed"] += 1
        
        # è®¡ç®—é›†æˆæµ‹è¯•ç»“æœ
        integration_results["total_tests"] = len(integration_results["test_cases"])
        integration_results["passed"] = sum(1 for case in integration_results["test_cases"] if case["success"])
        integration_results["failed"] = integration_results["total_tests"] - integration_results["passed"]
        
        self.test_results["test_suites"]["integration_tests"] = integration_results
        self.logger.info(f"ğŸ”— é›†æˆæµ‹è¯•å®Œæˆ: {integration_results['passed']}/{integration_results['total_tests']} é€šè¿‡")
    
    async def _run_api_tests(self):
        """è¿è¡ŒAPIæµ‹è¯•"""
        self.logger.info("ğŸŒ å¼€å§‹APIæµ‹è¯•")
        self.logger.info("-" * 40)
        
        api_results = {
            "total_tests": 0,
            "passed": 0,
            "failed": 0,
            "test_cases": []
        }
        
        try:
            api_test = TestAPIEndpoints()
            await api_test.setUp()
            
            # æµ‹è¯•APIç«¯ç‚¹
            api_endpoints = [
                ("å¥åº·æ£€æŸ¥", "/health"),
                ("å‘½ä»¤æ‰§è¡Œ", "/commands/execute"),
                ("å¹¶è¡Œå‘½ä»¤æ‰§è¡Œ", "/commands/execute-parallel"),
                ("æ™ºèƒ½ä½“çŠ¶æ€", "/agents/status"),
                ("MCPçŠ¶æ€", "/mcp/status")
            ]
            
            for endpoint_name, endpoint_path in api_endpoints:
                self.logger.info(f"ğŸ”Œ æµ‹è¯• {endpoint_name} API")
                
                try:
                    result = await api_test.test_endpoint(endpoint_path)
                    api_results["test_cases"].append({
                        "name": endpoint_name,
                        "endpoint": endpoint_path,
                        "result": result,
                        "success": result.get("status_code") == 200
                    })
                except Exception as e:
                    self.logger.error(f"   âŒ APIæµ‹è¯•å¤±è´¥: {e}")
                    api_results["test_cases"].append({
                        "name": endpoint_name,
                        "endpoint": endpoint_path,
                        "result": {"error": str(e)},
                        "success": False
                    })
        
        except Exception as e:
            self.logger.error(f"APIæµ‹è¯•åˆå§‹åŒ–å¤±è´¥: {e}")
            api_results["failed"] += 1
        
        # è®¡ç®—APIæµ‹è¯•ç»“æœ
        api_results["total_tests"] = len(api_results["test_cases"])
        api_results["passed"] = sum(1 for case in api_results["test_cases"] if case["success"])
        api_results["failed"] = api_results["total_tests"] - api_results["passed"]
        
        self.test_results["test_suites"]["api_tests"] = api_results
        self.logger.info(f"ğŸŒ APIæµ‹è¯•å®Œæˆ: {api_results['passed']}/{api_results['total_tests']} é€šè¿‡")
    
    async def _run_performance_tests(self):
        """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
        self.logger.info("âš¡ å¼€å§‹æ€§èƒ½æµ‹è¯•")
        self.logger.info("-" * 40)
        
        performance_results = {
            "total_tests": 0,
            "passed": 0,
            "failed": 0,
            "metrics": {},
            "test_cases": []
        }
        
        try:
            perf_test = TestPerformance()
            await perf_test.setUp()
            
            # å¹¶å‘æ€§èƒ½æµ‹è¯•
            self.logger.info("ğŸš€ å¹¶å‘å¤„ç†æ€§èƒ½æµ‹è¯•")
            concurrency_result = await perf_test.test_concurrent_processing()
            performance_results["test_cases"].append({
                "name": "å¹¶å‘å¤„ç†æ€§èƒ½",
                "result": concurrency_result,
                "success": concurrency_result.get("success", False)
            })
            
            # å“åº”æ—¶é—´æµ‹è¯•
            self.logger.info("â±ï¸ å“åº”æ—¶é—´æµ‹è¯•")
            response_time_result = await perf_test.test_response_time()
            performance_results["test_cases"].append({
                "name": "å“åº”æ—¶é—´",
                "result": response_time_result,
                "success": response_time_result.get("success", False)
            })
            
            # å†…å­˜ä½¿ç”¨æµ‹è¯•
            self.logger.info("ğŸ’¾ å†…å­˜ä½¿ç”¨æµ‹è¯•")
            memory_result = await perf_test.test_memory_usage()
            performance_results["test_cases"].append({
                "name": "å†…å­˜ä½¿ç”¨",
                "result": memory_result,
                "success": memory_result.get("success", False)
            })
            
            # æ±‡æ€»æ€§èƒ½æŒ‡æ ‡
            performance_results["metrics"] = {
                "average_response_time": response_time_result.get("average_time", 0),
                "max_concurrent_tasks": concurrency_result.get("max_concurrent", 0),
                "memory_usage_mb": memory_result.get("peak_memory_mb", 0),
                "throughput_per_second": concurrency_result.get("throughput", 0)
            }
            
        except Exception as e:
            self.logger.error(f"æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
            performance_results["failed"] += 1
        
        # è®¡ç®—æ€§èƒ½æµ‹è¯•ç»“æœ
        performance_results["total_tests"] = len(performance_results["test_cases"])
        performance_results["passed"] = sum(1 for case in performance_results["test_cases"] if case["success"])
        performance_results["failed"] = performance_results["total_tests"] - performance_results["passed"]
        
        self.test_results["test_suites"]["performance_tests"] = performance_results
        self.test_results["performance_metrics"] = performance_results["metrics"]
        
        self.logger.info(f"âš¡ æ€§èƒ½æµ‹è¯•å®Œæˆ: {performance_results['passed']}/{performance_results['total_tests']} é€šè¿‡")
    
    async def _test_end_to_end_workflow(self) -> Dict[str, Any]:
        """ç«¯åˆ°ç«¯å·¥ä½œæµæµ‹è¯•"""
        try:
            self.logger.info("ğŸ”„ æ‰§è¡Œç«¯åˆ°ç«¯å·¥ä½œæµæµ‹è¯•")
            
            # æ¨¡æ‹Ÿå®Œæ•´çš„å·¥ä½œæµï¼šæ¶æ„è®¾è®¡ -> å¼€å‘ -> æµ‹è¯• -> éƒ¨ç½²
            workflow_steps = [
                "æ™ºæ…§è·¯ç”±æ¥æ”¶è¯·æ±‚",
                "MCPåè°ƒå™¨åˆ†é…ä»»åŠ¡",
                "æ¶æ„å¸ˆæ™ºèƒ½ä½“è®¾è®¡æ¶æ„",
                "å¼€å‘æ™ºèƒ½ä½“å®ç°åŠŸèƒ½",
                "æµ‹è¯•æ™ºèƒ½ä½“éªŒè¯è´¨é‡",
                "éƒ¨ç½²æ™ºèƒ½ä½“å‘å¸ƒç³»ç»Ÿ"
            ]
            
            completed_steps = []
            
            for step in workflow_steps:
                # æ¨¡æ‹Ÿæ¯ä¸ªæ­¥éª¤çš„æ‰§è¡Œ
                await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
                completed_steps.append(step)
                self.logger.info(f"   âœ… {step}")
            
            return {
                "status": "success",
                "completed_steps": completed_steps,
                "total_steps": len(workflow_steps),
                "success_rate": len(completed_steps) / len(workflow_steps)
            }
            
        except Exception as e:
            return {
                "status": "failed",
                "error": str(e),
                "completed_steps": completed_steps,
                "total_steps": len(workflow_steps)
            }
    
    async def _generate_test_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        self.logger.info("ğŸ“Š ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š")
        
        report_data = {
            "test_run_info": {
                "start_time": self.test_results["start_time"].isoformat(),
                "end_time": self.test_results["end_time"].isoformat() if self.test_results["end_time"] else None,
                "duration": self.test_results["total_duration"],
                "environment": "PowerAutomation 4.0 Development"
            },
            "test_results": self.test_results["test_suites"],
            "summary": self.test_results["summary"],
            "performance_metrics": self.test_results["performance_metrics"]
        }
        
        # ä¿å­˜JSONæŠ¥å‘Š
        report_file = f"tests/reports/test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        os.makedirs(os.path.dirname(report_file), exist_ok=True)
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False, default=str)
        
        self.logger.info(f"ğŸ“Š æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    def _calculate_summary(self):
        """è®¡ç®—æ€»ä½“æµ‹è¯•ç»“æœ"""
        summary = self.test_results["summary"]
        
        for suite_name, suite_results in self.test_results["test_suites"].items():
            summary["total_tests"] += suite_results.get("total_tests", 0)
            summary["passed"] += suite_results.get("passed", 0)
            summary["failed"] += suite_results.get("failed", 0)
            summary["skipped"] += suite_results.get("skipped", 0)
    
    async def _print_final_results(self):
        """è¾“å‡ºæœ€ç»ˆæµ‹è¯•ç»“æœ"""
        self.logger.info("=" * 80)
        self.logger.info("ğŸ¯ PowerAutomation 4.0 æµ‹è¯•ç»“æœæ±‡æ€»")
        self.logger.info("=" * 80)
        
        summary = self.test_results["summary"]
        
        # æ€»ä½“ç»Ÿè®¡
        self.logger.info(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        self.logger.info(f"   æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
        self.logger.info(f"   é€šè¿‡: {summary['passed']} âœ…")
        self.logger.info(f"   å¤±è´¥: {summary['failed']} âŒ")
        self.logger.info(f"   è·³è¿‡: {summary['skipped']} â­ï¸")
        self.logger.info(f"   é”™è¯¯: {summary['errors']} ğŸ’¥")
        
        # æˆåŠŸç‡
        if summary['total_tests'] > 0:
            success_rate = (summary['passed'] / summary['total_tests']) * 100
            self.logger.info(f"   æˆåŠŸç‡: {success_rate:.1f}%")
        
        # æ‰§è¡Œæ—¶é—´
        self.logger.info(f"   æ‰§è¡Œæ—¶é—´: {self.test_results['total_duration']:.2f} ç§’")
        
        # å„æµ‹è¯•å¥—ä»¶ç»“æœ
        self.logger.info(f"\\nğŸ“‹ å„æµ‹è¯•å¥—ä»¶ç»“æœ:")
        for suite_name, suite_results in self.test_results["test_suites"].items():
            passed = suite_results.get("passed", 0)
            total = suite_results.get("total_tests", 0)
            status = "âœ…" if passed == total and total > 0 else "âŒ"
            self.logger.info(f"   {suite_name}: {passed}/{total} {status}")
        
        # æ€§èƒ½æŒ‡æ ‡
        if self.test_results["performance_metrics"]:
            self.logger.info(f"\\nâš¡ æ€§èƒ½æŒ‡æ ‡:")
            metrics = self.test_results["performance_metrics"]
            self.logger.info(f"   å¹³å‡å“åº”æ—¶é—´: {metrics.get('average_response_time', 0):.3f} ç§’")
            self.logger.info(f"   æœ€å¤§å¹¶å‘ä»»åŠ¡: {metrics.get('max_concurrent_tasks', 0)}")
            self.logger.info(f"   å†…å­˜ä½¿ç”¨å³°å€¼: {metrics.get('memory_usage_mb', 0):.1f} MB")
            self.logger.info(f"   ååé‡: {metrics.get('throughput_per_second', 0):.1f} ä»»åŠ¡/ç§’")
        
        # æœ€ç»ˆåˆ¤æ–­
        overall_success = summary['failed'] == 0 and summary['errors'] == 0 and summary['total_tests'] > 0
        final_status = "ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼" if overall_success else "âš ï¸ å­˜åœ¨æµ‹è¯•å¤±è´¥"
        self.logger.info(f"\\n{final_status}")
        self.logger.info("=" * 80)

async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='PowerAutomation 4.0 æµ‹è¯•è¿è¡Œå™¨')
    parser.add_argument('--types', nargs='+', choices=['unit', 'integration', 'api', 'performance'], 
                       help='è¦è¿è¡Œçš„æµ‹è¯•ç±»å‹')
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†è¾“å‡º')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # åˆ›å»ºæµ‹è¯•è¿è¡Œå™¨
    runner = PowerAutomationTestRunner()
    
    # è¿è¡Œæµ‹è¯•
    results = await runner.run_all_tests(test_types=args.types)
    
    # è¿”å›é€€å‡ºç 
    exit_code = 0 if results["summary"]["failed"] == 0 and results["summary"]["errors"] == 0 else 1
    sys.exit(exit_code)

if __name__ == "__main__":
    asyncio.run(main())

