#!/usr/bin/env python3
"""
ç›¸ä¼¼æ€§åŒ¹é…å™¨

é«˜çº§å·¥å…·åŒ¹é…ç®—æ³•ï¼ŒåŸºäºå¤šç§ç›¸ä¼¼æ€§è®¡ç®—æ–¹æ³•ä¸ºä»»åŠ¡éœ€æ±‚æ‰¾åˆ°æœ€åŒ¹é…çš„å·¥å…·ã€‚
æ”¯æŒè¯­ä¹‰ç›¸ä¼¼æ€§ã€åŠŸèƒ½ç›¸ä¼¼æ€§ã€ç»“æ„ç›¸ä¼¼æ€§ç­‰å¤šç»´åº¦åŒ¹é…ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
- å¤šç»´åº¦ç›¸ä¼¼æ€§è®¡ç®—
- è¯­ä¹‰åµŒå…¥åŒ¹é…
- åŠŸèƒ½ç‰¹å¾åŒ¹é…
- ç»“æ„ç›¸ä¼¼æ€§åˆ†æ
- æ··åˆåŒ¹é…ç­–ç•¥
- åŒ¹é…ç»“æœè§£é‡Š

ä½œè€…: PowerAutomation Team
ç‰ˆæœ¬: 4.1.0
æ—¥æœŸ: 2025-01-07
"""

import asyncio
import json
import numpy as np
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass, field
from enum import Enum
import hashlib
import re
from pathlib import Path
import pickle
from collections import defaultdict
import math

from ..models.tool_models import MCPTool, TaskRequirement, ToolCapability, ToolCategory

logger = logging.getLogger(__name__)

class SimilarityType(Enum):
    """ç›¸ä¼¼æ€§ç±»å‹"""
    SEMANTIC = "semantic"  # è¯­ä¹‰ç›¸ä¼¼æ€§
    FUNCTIONAL = "functional"  # åŠŸèƒ½ç›¸ä¼¼æ€§
    STRUCTURAL = "structural"  # ç»“æ„ç›¸ä¼¼æ€§
    CONTEXTUAL = "contextual"  # ä¸Šä¸‹æ–‡ç›¸ä¼¼æ€§
    BEHAVIORAL = "behavioral"  # è¡Œä¸ºç›¸ä¼¼æ€§
    HYBRID = "hybrid"  # æ··åˆç›¸ä¼¼æ€§

class MatchingStrategy(Enum):
    """åŒ¹é…ç­–ç•¥"""
    EXACT_MATCH = "exact_match"  # ç²¾ç¡®åŒ¹é…
    FUZZY_MATCH = "fuzzy_match"  # æ¨¡ç³ŠåŒ¹é…
    SEMANTIC_MATCH = "semantic_match"  # è¯­ä¹‰åŒ¹é…
    FEATURE_MATCH = "feature_match"  # ç‰¹å¾åŒ¹é…
    ENSEMBLE_MATCH = "ensemble_match"  # é›†æˆåŒ¹é…

@dataclass
class SimilarityScore:
    """ç›¸ä¼¼æ€§è¯„åˆ†"""
    tool_id: str
    task_id: str
    similarity_type: SimilarityType
    score: float
    confidence: float
    details: Dict[str, Any] = field(default_factory=dict)
    explanation: str = ""
    computed_at: datetime = field(default_factory=datetime.now)

@dataclass
class MatchingResult:
    """åŒ¹é…ç»“æœ"""
    tool_id: str
    task_id: str
    overall_similarity: float
    similarity_breakdown: Dict[SimilarityType, float] = field(default_factory=dict)
    confidence: float = 0.0
    rank: int = 0
    explanation: str = ""
    matching_features: List[str] = field(default_factory=list)
    missing_features: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class FeatureVector:
    """ç‰¹å¾å‘é‡"""
    tool_id: str
    features: Dict[str, float] = field(default_factory=dict)
    embedding: Optional[np.ndarray] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    created_at: datetime = field(default_factory=datetime.now)

class SimilarityMatcher:
    """ç›¸ä¼¼æ€§åŒ¹é…å™¨"""
    
    def __init__(self, config_path: str = "./similarity_matcher_config.json"):
        """åˆå§‹åŒ–ç›¸ä¼¼æ€§åŒ¹é…å™¨"""
        self.config_path = Path(config_path)
        self.config = self._load_config()
        
        # åŒ¹é…å‚æ•°
        self.similarity_threshold = self.config.get("similarity_threshold", 0.6)
        self.confidence_threshold = self.config.get("confidence_threshold", 0.7)
        self.max_results = self.config.get("max_results", 20)
        
        # æƒé‡é…ç½®
        self.similarity_weights = self.config.get("similarity_weights", {
            "semantic": 0.3,
            "functional": 0.25,
            "structural": 0.2,
            "contextual": 0.15,
            "behavioral": 0.1
        })
        
        # ç‰¹å¾æå–
        self.feature_extractors = {
            "capabilities": self._extract_capability_features,
            "description": self._extract_description_features,
            "category": self._extract_category_features,
            "dependencies": self._extract_dependency_features,
            "metadata": self._extract_metadata_features
        }
        
        # ç¼“å­˜
        self.feature_cache: Dict[str, FeatureVector] = {}
        self.similarity_cache: Dict[str, List[SimilarityScore]] = {}
        self.embedding_cache: Dict[str, np.ndarray] = {}
        
        # å­˜å‚¨
        self.data_dir = Path(self.config.get("data_dir", "./similarity_data"))
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½æŒä¹…åŒ–æ•°æ®
        self._load_persistent_data()
        
        logger.info("ç›¸ä¼¼æ€§åŒ¹é…å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®"""
        default_config = {
            "similarity_threshold": 0.6,
            "confidence_threshold": 0.7,
            "max_results": 20,
            "similarity_weights": {
                "semantic": 0.3,
                "functional": 0.25,
                "structural": 0.2,
                "contextual": 0.15,
                "behavioral": 0.1
            },
            "enable_caching": True,
            "cache_ttl": 3600,
            "data_dir": "./similarity_data",
            "embedding_dimension": 384,
            "feature_normalization": True,
            "enable_fuzzy_matching": True,
            "fuzzy_threshold": 0.8
        }
        
        if self.config_path.exists():
            try:
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                default_config.update(user_config)
            except Exception as e:
                logger.warning(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
        
        return default_config
    
    def _load_persistent_data(self):
        """åŠ è½½æŒä¹…åŒ–æ•°æ®"""
        try:
            # åŠ è½½ç‰¹å¾ç¼“å­˜
            feature_cache_file = self.data_dir / "feature_cache.pkl"
            if feature_cache_file.exists():
                with open(feature_cache_file, 'rb') as f:
                    self.feature_cache = pickle.load(f)
            
            # åŠ è½½åµŒå…¥ç¼“å­˜
            embedding_cache_file = self.data_dir / "embedding_cache.pkl"
            if embedding_cache_file.exists():
                with open(embedding_cache_file, 'rb') as f:
                    self.embedding_cache = pickle.load(f)
            
            logger.info("æŒä¹…åŒ–æ•°æ®åŠ è½½å®Œæˆ")
            
        except Exception as e:
            logger.warning(f"åŠ è½½æŒä¹…åŒ–æ•°æ®å¤±è´¥: {e}")
    
    def _save_persistent_data(self):
        """ä¿å­˜æŒä¹…åŒ–æ•°æ®"""
        try:
            # ä¿å­˜ç‰¹å¾ç¼“å­˜
            with open(self.data_dir / "feature_cache.pkl", 'wb') as f:
                pickle.dump(self.feature_cache, f)
            
            # ä¿å­˜åµŒå…¥ç¼“å­˜
            with open(self.data_dir / "embedding_cache.pkl", 'wb') as f:
                pickle.dump(self.embedding_cache, f)
            
            logger.debug("æŒä¹…åŒ–æ•°æ®ä¿å­˜å®Œæˆ")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æŒä¹…åŒ–æ•°æ®å¤±è´¥: {e}")
    
    async def compute_similarity(self, task_requirement: TaskRequirement,
                               tools: List[MCPTool],
                               similarity_type: SimilarityType = SimilarityType.HYBRID,
                               strategy: MatchingStrategy = MatchingStrategy.ENSEMBLE_MATCH) -> List[MatchingResult]:
        """è®¡ç®—ç›¸ä¼¼æ€§"""
        try:
            # æå–ä»»åŠ¡ç‰¹å¾
            task_features = await self._extract_task_features(task_requirement)
            
            # è®¡ç®—å·¥å…·ç›¸ä¼¼æ€§
            matching_results = []
            
            for tool in tools:
                # æå–å·¥å…·ç‰¹å¾
                tool_features = await self._extract_tool_features(tool)
                
                # è®¡ç®—ç›¸ä¼¼æ€§è¯„åˆ†
                similarity_scores = await self._compute_tool_similarity(
                    task_features, tool_features, tool, task_requirement, similarity_type
                )
                
                # ç”ŸæˆåŒ¹é…ç»“æœ
                matching_result = await self._generate_matching_result(
                    tool, task_requirement, similarity_scores, strategy
                )
                
                if matching_result.overall_similarity >= self.similarity_threshold:
                    matching_results.append(matching_result)
            
            # æ’åºå’Œæ’å
            matching_results.sort(key=lambda x: x.overall_similarity, reverse=True)
            for i, result in enumerate(matching_results):
                result.rank = i + 1
            
            # é™åˆ¶ç»“æœæ•°é‡
            matching_results = matching_results[:self.max_results]
            
            logger.info(f"è®¡ç®—ç›¸ä¼¼æ€§å®Œæˆï¼Œæ‰¾åˆ° {len(matching_results)} ä¸ªåŒ¹é…å·¥å…·")
            return matching_results
            
        except Exception as e:
            logger.error(f"è®¡ç®—ç›¸ä¼¼æ€§å¤±è´¥: {e}")
            raise
    
    async def _extract_task_features(self, task_requirement: TaskRequirement) -> FeatureVector:
        """æå–ä»»åŠ¡ç‰¹å¾"""
        task_id = task_requirement.task_id
        
        # æ£€æŸ¥ç¼“å­˜
        if task_id in self.feature_cache:
            return self.feature_cache[task_id]
        
        features = {}
        
        # æè¿°ç‰¹å¾
        if task_requirement.description:
            desc_features = await self._extract_description_features(task_requirement.description)
            features.update(desc_features)
        
        # èƒ½åŠ›ç‰¹å¾
        if task_requirement.required_capabilities:
            cap_features = await self._extract_capability_features(task_requirement.required_capabilities)
            features.update(cap_features)
        
        # å¤æ‚åº¦ç‰¹å¾
        if task_requirement.complexity_level:
            complexity_features = await self._extract_complexity_features(task_requirement.complexity_level)
            features.update(complexity_features)
        
        # ä¸Šä¸‹æ–‡ç‰¹å¾
        if hasattr(task_requirement, 'context') and task_requirement.context:
            context_features = await self._extract_context_features(task_requirement.context)
            features.update(context_features)
        
        # ç”ŸæˆåµŒå…¥
        text_content = f"{task_requirement.description} {' '.join(task_requirement.required_capabilities or [])}"
        embedding = await self._get_text_embedding(text_content)
        
        # åˆ›å»ºç‰¹å¾å‘é‡
        feature_vector = FeatureVector(
            tool_id=task_id,
            features=features,
            embedding=embedding,
            metadata={
                "type": "task",
                "complexity": task_requirement.complexity_level,
                "capabilities_count": len(task_requirement.required_capabilities or [])
            }
        )
        
        # ç¼“å­˜ç‰¹å¾
        self.feature_cache[task_id] = feature_vector
        
        return feature_vector
    
    async def _extract_tool_features(self, tool: MCPTool) -> FeatureVector:
        """æå–å·¥å…·ç‰¹å¾"""
        tool_id = tool.tool_id
        
        # æ£€æŸ¥ç¼“å­˜
        if tool_id in self.feature_cache:
            return self.feature_cache[tool_id]
        
        features = {}
        
        # ä½¿ç”¨ç‰¹å¾æå–å™¨
        for extractor_name, extractor_func in self.feature_extractors.items():
            try:
                extracted_features = await extractor_func(tool)
                features.update(extracted_features)
            except Exception as e:
                logger.debug(f"ç‰¹å¾æå–å¤±è´¥ {extractor_name}: {e}")
        
        # ç”ŸæˆåµŒå…¥
        text_content = f"{tool.name} {tool.description} {' '.join([cap.name for cap in tool.capabilities])}"
        embedding = await self._get_text_embedding(text_content)
        
        # åˆ›å»ºç‰¹å¾å‘é‡
        feature_vector = FeatureVector(
            tool_id=tool_id,
            features=features,
            embedding=embedding,
            metadata={
                "type": "tool",
                "category": tool.category.value,
                "capabilities_count": len(tool.capabilities),
                "status": tool.status.value
            }
        )
        
        # ç¼“å­˜ç‰¹å¾
        self.feature_cache[tool_id] = feature_vector
        
        return feature_vector
    
    async def _extract_capability_features(self, capabilities) -> Dict[str, float]:
        """æå–èƒ½åŠ›ç‰¹å¾"""
        features = {}
        
        if isinstance(capabilities, list):
            # ä»»åŠ¡éœ€æ±‚çš„èƒ½åŠ›åˆ—è¡¨
            for cap in capabilities:
                features[f"capability_{cap}"] = 1.0
                
                # èƒ½åŠ›ç±»åˆ«ç‰¹å¾
                category = self._categorize_capability(cap)
                features[f"capability_category_{category}"] = 1.0
        
        elif hasattr(capabilities, '__iter__'):
            # å·¥å…·çš„èƒ½åŠ›å¯¹è±¡åˆ—è¡¨
            for cap in capabilities:
                if hasattr(cap, 'name'):
                    features[f"capability_{cap.name}"] = getattr(cap, 'confidence', 1.0)
                    
                    # èƒ½åŠ›ç±»åˆ«ç‰¹å¾
                    category = self._categorize_capability(cap.name)
                    features[f"capability_category_{category}"] = getattr(cap, 'confidence', 1.0)
        
        return features
    
    def _categorize_capability(self, capability_name: str) -> str:
        """èƒ½åŠ›åˆ†ç±»"""
        capability_categories = {
            "file": ["file", "directory", "path", "read", "write"],
            "web": ["web", "http", "url", "scraping", "browser"],
            "data": ["data", "json", "csv", "database", "query"],
            "ai": ["ai", "ml", "model", "prediction", "analysis"],
            "system": ["system", "process", "command", "shell"],
            "network": ["network", "api", "request", "socket"],
            "text": ["text", "string", "parse", "format"],
            "image": ["image", "photo", "picture", "visual"],
            "audio": ["audio", "sound", "music", "voice"],
            "video": ["video", "movie", "stream", "media"]
        }
        
        capability_lower = capability_name.lower()
        
        for category, keywords in capability_categories.items():
            if any(keyword in capability_lower for keyword in keywords):
                return category
        
        return "general"
    
    async def _extract_description_features(self, description) -> Dict[str, float]:
        """æå–æè¿°ç‰¹å¾"""
        if isinstance(description, str):
            text = description
        elif hasattr(description, 'description'):
            text = description.description
        else:
            text = str(description)
        
        features = {}
        
        # æ–‡æœ¬é•¿åº¦ç‰¹å¾
        features["description_length"] = min(len(text) / 1000, 1.0)  # å½’ä¸€åŒ–åˆ°0-1
        
        # å…³é”®è¯ç‰¹å¾
        keywords = self._extract_keywords(text)
        for keyword in keywords:
            features[f"keyword_{keyword}"] = 1.0
        
        # æŠ€æœ¯æ ˆç‰¹å¾
        tech_stack = self._identify_tech_stack(text)
        for tech in tech_stack:
            features[f"tech_{tech}"] = 1.0
        
        # åŠ¨ä½œè¯ç‰¹å¾
        actions = self._extract_action_words(text)
        for action in actions:
            features[f"action_{action}"] = 1.0
        
        return features
    
    def _extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯"""
        # ç®€åŒ–çš„å…³é”®è¯æå–
        words = re.findall(r'\b\w+\b', text.lower())
        
        # è¿‡æ»¤åœç”¨è¯
        stop_words = {"the", "a", "an", "and", "or", "but", "in", "on", "at", "to", "for", "of", "with", "by"}
        keywords = [word for word in words if word not in stop_words and len(word) > 2]
        
        # è¿”å›é¢‘ç‡æœ€é«˜çš„å…³é”®è¯
        from collections import Counter
        word_counts = Counter(keywords)
        return [word for word, count in word_counts.most_common(10)]
    
    def _identify_tech_stack(self, text: str) -> List[str]:
        """è¯†åˆ«æŠ€æœ¯æ ˆ"""
        tech_patterns = {
            "python": r"\bpython\b",
            "javascript": r"\b(javascript|js|node)\b",
            "react": r"\breact\b",
            "flask": r"\bflask\b",
            "docker": r"\bdocker\b",
            "api": r"\bapi\b",
            "rest": r"\brest\b",
            "json": r"\bjson\b",
            "sql": r"\bsql\b",
            "mongodb": r"\bmongodb\b",
            "redis": r"\bredis\b",
            "aws": r"\baws\b",
            "azure": r"\bazure\b",
            "gcp": r"\bgcp\b"
        }
        
        identified_tech = []
        text_lower = text.lower()
        
        for tech, pattern in tech_patterns.items():
            if re.search(pattern, text_lower):
                identified_tech.append(tech)
        
        return identified_tech
    
    def _extract_action_words(self, text: str) -> List[str]:
        """æå–åŠ¨ä½œè¯"""
        action_patterns = [
            r"\b(create|build|generate|make)\b",
            r"\b(read|parse|extract|analyze)\b",
            r"\b(update|modify|change|edit)\b",
            r"\b(delete|remove|clean)\b",
            r"\b(send|post|get|fetch)\b",
            r"\b(process|handle|manage)\b",
            r"\b(convert|transform|format)\b",
            r"\b(validate|verify|check)\b"
        ]
        
        actions = []
        text_lower = text.lower()
        
        for pattern in action_patterns:
            matches = re.findall(pattern, text_lower)
            actions.extend(matches)
        
        return list(set(actions))
    
    async def _extract_category_features(self, tool: MCPTool) -> Dict[str, float]:
        """æå–ç±»åˆ«ç‰¹å¾"""
        features = {}
        
        # ä¸»ç±»åˆ«
        features[f"category_{tool.category.value}"] = 1.0
        
        # ç±»åˆ«å±‚æ¬¡ç‰¹å¾
        category_hierarchy = {
            ToolCategory.DEVELOPMENT: ["development", "coding", "programming"],
            ToolCategory.DATA_PROCESSING: ["data", "processing", "analysis"],
            ToolCategory.WEB_AUTOMATION: ["web", "automation", "browser"],
            ToolCategory.UTILITY: ["utility", "tool", "helper"],
            ToolCategory.AI_ML: ["ai", "ml", "machine_learning", "artificial_intelligence"]
        }
        
        if tool.category in category_hierarchy:
            for subcategory in category_hierarchy[tool.category]:
                features[f"subcategory_{subcategory}"] = 1.0
        
        return features
    
    async def _extract_dependency_features(self, tool: MCPTool) -> Dict[str, float]:
        """æå–ä¾èµ–ç‰¹å¾"""
        features = {}
        
        if tool.dependencies:
            # ä¾èµ–æ•°é‡
            features["dependency_count"] = min(len(tool.dependencies) / 10, 1.0)
            
            # ä¾èµ–ç±»å‹
            for dep in tool.dependencies:
                dep_type = self._classify_dependency(dep)
                features[f"dependency_type_{dep_type}"] = 1.0
        else:
            features["dependency_count"] = 0.0
            features["no_dependencies"] = 1.0
        
        return features
    
    def _classify_dependency(self, dependency: str) -> str:
        """åˆ†ç±»ä¾èµ–"""
        dep_lower = dependency.lower()
        
        if any(lang in dep_lower for lang in ["python", "pip", "pypi"]):
            return "python"
        elif any(js in dep_lower for js in ["npm", "node", "javascript"]):
            return "javascript"
        elif any(sys in dep_lower for sys in ["system", "os", "linux", "windows"]):
            return "system"
        elif any(db in dep_lower for db in ["database", "sql", "mongodb", "redis"]):
            return "database"
        else:
            return "other"
    
    async def _extract_metadata_features(self, tool: MCPTool) -> Dict[str, float]:
        """æå–å…ƒæ•°æ®ç‰¹å¾"""
        features = {}
        
        # ç‰ˆæœ¬ç‰¹å¾
        if hasattr(tool, 'version') and tool.version:
            try:
                version_parts = tool.version.split('.')
                major_version = int(version_parts[0])
                features["version_major"] = min(major_version / 10, 1.0)
                features["is_stable"] = 1.0 if major_version >= 1 else 0.0
            except:
                features["version_unknown"] = 1.0
        
        # æ–‡æ¡£ç‰¹å¾
        features["has_documentation"] = 1.0 if tool.documentation_url else 0.0
        features["has_examples"] = 1.0 if tool.examples else 0.0
        
        # ä»“åº“ç‰¹å¾
        features["has_repository"] = 1.0 if tool.repository_url else 0.0
        
        # çŠ¶æ€ç‰¹å¾
        features[f"status_{tool.status.value}"] = 1.0
        
        return features
    
    async def _extract_complexity_features(self, complexity_level: str) -> Dict[str, float]:
        """æå–å¤æ‚åº¦ç‰¹å¾"""
        features = {}
        
        complexity_mapping = {
            "low": 0.2,
            "medium": 0.5,
            "high": 0.8,
            "very_high": 1.0
        }
        
        features["complexity_level"] = complexity_mapping.get(complexity_level, 0.5)
        features[f"complexity_{complexity_level}"] = 1.0
        
        return features
    
    async def _extract_context_features(self, context: Dict[str, Any]) -> Dict[str, float]:
        """æå–ä¸Šä¸‹æ–‡ç‰¹å¾"""
        features = {}
        
        # ç¯å¢ƒç‰¹å¾
        if "environment" in context:
            env = context["environment"]
            if isinstance(env, str):
                features[f"environment_{env}"] = 1.0
        
        # å¹³å°ç‰¹å¾
        if "platform" in context:
            platform = context["platform"]
            if isinstance(platform, str):
                features[f"platform_{platform}"] = 1.0
        
        # ç”¨æˆ·ç±»å‹ç‰¹å¾
        if "user_type" in context:
            user_type = context["user_type"]
            if isinstance(user_type, str):
                features[f"user_type_{user_type}"] = 1.0
        
        return features
    
    async def _get_text_embedding(self, text: str) -> Optional[np.ndarray]:
        """è·å–æ–‡æœ¬åµŒå…¥"""
        # ç®€åŒ–å®ç°ï¼šä½¿ç”¨ç¼“å­˜çš„éšæœºåµŒå…¥
        text_hash = hashlib.md5(text.encode()).hexdigest()
        
        if text_hash not in self.embedding_cache:
            # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥è°ƒç”¨çœŸå®çš„åµŒå…¥æ¨¡å‹
            np.random.seed(int(text_hash[:8], 16))
            embedding_dim = self.config.get("embedding_dimension", 384)
            embedding = np.random.normal(0, 1, embedding_dim)
            embedding = embedding / np.linalg.norm(embedding)  # å½’ä¸€åŒ–
            self.embedding_cache[text_hash] = embedding
        
        return self.embedding_cache[text_hash]
    
    async def _compute_tool_similarity(self, task_features: FeatureVector,
                                     tool_features: FeatureVector,
                                     tool: MCPTool,
                                     task_requirement: TaskRequirement,
                                     similarity_type: SimilarityType) -> Dict[SimilarityType, SimilarityScore]:
        """è®¡ç®—å·¥å…·ç›¸ä¼¼æ€§"""
        similarity_scores = {}
        
        if similarity_type == SimilarityType.HYBRID:
            # è®¡ç®—æ‰€æœ‰ç±»å‹çš„ç›¸ä¼¼æ€§
            for sim_type in [SimilarityType.SEMANTIC, SimilarityType.FUNCTIONAL, 
                           SimilarityType.STRUCTURAL, SimilarityType.CONTEXTUAL, 
                           SimilarityType.BEHAVIORAL]:
                score = await self._compute_single_similarity(
                    task_features, tool_features, tool, task_requirement, sim_type
                )
                similarity_scores[sim_type] = score
        else:
            # è®¡ç®—å•ä¸€ç±»å‹çš„ç›¸ä¼¼æ€§
            score = await self._compute_single_similarity(
                task_features, tool_features, tool, task_requirement, similarity_type
            )
            similarity_scores[similarity_type] = score
        
        return similarity_scores
    
    async def _compute_single_similarity(self, task_features: FeatureVector,
                                       tool_features: FeatureVector,
                                       tool: MCPTool,
                                       task_requirement: TaskRequirement,
                                       similarity_type: SimilarityType) -> SimilarityScore:
        """è®¡ç®—å•ä¸€ç›¸ä¼¼æ€§"""
        if similarity_type == SimilarityType.SEMANTIC:
            return await self._compute_semantic_similarity(task_features, tool_features, tool, task_requirement)
        elif similarity_type == SimilarityType.FUNCTIONAL:
            return await self._compute_functional_similarity(task_features, tool_features, tool, task_requirement)
        elif similarity_type == SimilarityType.STRUCTURAL:
            return await self._compute_structural_similarity(task_features, tool_features, tool, task_requirement)
        elif similarity_type == SimilarityType.CONTEXTUAL:
            return await self._compute_contextual_similarity(task_features, tool_features, tool, task_requirement)
        elif similarity_type == SimilarityType.BEHAVIORAL:
            return await self._compute_behavioral_similarity(task_features, tool_features, tool, task_requirement)
        else:
            # é»˜è®¤è¿”å›åŠŸèƒ½ç›¸ä¼¼æ€§
            return await self._compute_functional_similarity(task_features, tool_features, tool, task_requirement)
    
    async def _compute_semantic_similarity(self, task_features: FeatureVector,
                                         tool_features: FeatureVector,
                                         tool: MCPTool,
                                         task_requirement: TaskRequirement) -> SimilarityScore:
        """è®¡ç®—è¯­ä¹‰ç›¸ä¼¼æ€§"""
        score = 0.0
        confidence = 0.0
        details = {}
        
        # åµŒå…¥ç›¸ä¼¼æ€§
        if task_features.embedding is not None and tool_features.embedding is not None:
            embedding_similarity = np.dot(task_features.embedding, tool_features.embedding)
            score += embedding_similarity * 0.6
            details["embedding_similarity"] = embedding_similarity
            confidence += 0.6
        
        # å…³é”®è¯ç›¸ä¼¼æ€§
        task_keywords = {k: v for k, v in task_features.features.items() if k.startswith("keyword_")}
        tool_keywords = {k: v for k, v in tool_features.features.items() if k.startswith("keyword_")}
        
        if task_keywords and tool_keywords:
            keyword_similarity = self._compute_feature_similarity(task_keywords, tool_keywords)
            score += keyword_similarity * 0.4
            details["keyword_similarity"] = keyword_similarity
            confidence += 0.4
        
        # å½’ä¸€åŒ–ç½®ä¿¡åº¦
        if confidence > 0:
            score = score / confidence
            confidence = min(confidence, 1.0)
        
        explanation = f"è¯­ä¹‰ç›¸ä¼¼æ€§åŸºäºæ–‡æœ¬åµŒå…¥({details.get('embedding_similarity', 0):.3f})å’Œå…³é”®è¯åŒ¹é…({details.get('keyword_similarity', 0):.3f})"
        
        return SimilarityScore(
            tool_id=tool.tool_id,
            task_id=task_requirement.task_id,
            similarity_type=SimilarityType.SEMANTIC,
            score=score,
            confidence=confidence,
            details=details,
            explanation=explanation
        )
    
    async def _compute_functional_similarity(self, task_features: FeatureVector,
                                           tool_features: FeatureVector,
                                           tool: MCPTool,
                                           task_requirement: TaskRequirement) -> SimilarityScore:
        """è®¡ç®—åŠŸèƒ½ç›¸ä¼¼æ€§"""
        score = 0.0
        confidence = 0.0
        details = {}
        
        # èƒ½åŠ›åŒ¹é…
        task_capabilities = {k: v for k, v in task_features.features.items() if k.startswith("capability_")}
        tool_capabilities = {k: v for k, v in tool_features.features.items() if k.startswith("capability_")}
        
        if task_capabilities and tool_capabilities:
            capability_similarity = self._compute_feature_similarity(task_capabilities, tool_capabilities)
            score += capability_similarity * 0.7
            details["capability_similarity"] = capability_similarity
            confidence += 0.7
        
        # åŠ¨ä½œåŒ¹é…
        task_actions = {k: v for k, v in task_features.features.items() if k.startswith("action_")}
        tool_actions = {k: v for k, v in tool_features.features.items() if k.startswith("action_")}
        
        if task_actions and tool_actions:
            action_similarity = self._compute_feature_similarity(task_actions, tool_actions)
            score += action_similarity * 0.3
            details["action_similarity"] = action_similarity
            confidence += 0.3
        
        # å½’ä¸€åŒ–
        if confidence > 0:
            score = score / confidence
            confidence = min(confidence, 1.0)
        
        explanation = f"åŠŸèƒ½ç›¸ä¼¼æ€§åŸºäºèƒ½åŠ›åŒ¹é…({details.get('capability_similarity', 0):.3f})å’ŒåŠ¨ä½œåŒ¹é…({details.get('action_similarity', 0):.3f})"
        
        return SimilarityScore(
            tool_id=tool.tool_id,
            task_id=task_requirement.task_id,
            similarity_type=SimilarityType.FUNCTIONAL,
            score=score,
            confidence=confidence,
            details=details,
            explanation=explanation
        )
    
    async def _compute_structural_similarity(self, task_features: FeatureVector,
                                           tool_features: FeatureVector,
                                           tool: MCPTool,
                                           task_requirement: TaskRequirement) -> SimilarityScore:
        """è®¡ç®—ç»“æ„ç›¸ä¼¼æ€§"""
        score = 0.0
        confidence = 0.0
        details = {}
        
        # å¤æ‚åº¦åŒ¹é…
        task_complexity = task_features.features.get("complexity_level", 0.5)
        tool_complexity = self._estimate_tool_complexity(tool)
        
        complexity_diff = abs(task_complexity - tool_complexity)
        complexity_similarity = 1.0 - complexity_diff
        score += complexity_similarity * 0.4
        details["complexity_similarity"] = complexity_similarity
        confidence += 0.4
        
        # ä¾èµ–åŒ¹é…
        task_deps = task_features.features.get("dependency_count", 0)
        tool_deps = tool_features.features.get("dependency_count", 0)
        
        if task_deps > 0 or tool_deps > 0:
            dep_diff = abs(task_deps - tool_deps)
            dep_similarity = 1.0 / (1.0 + dep_diff)
            score += dep_similarity * 0.3
            details["dependency_similarity"] = dep_similarity
            confidence += 0.3
        
        # æŠ€æœ¯æ ˆåŒ¹é…
        task_tech = {k: v for k, v in task_features.features.items() if k.startswith("tech_")}
        tool_tech = {k: v for k, v in tool_features.features.items() if k.startswith("tech_")}
        
        if task_tech and tool_tech:
            tech_similarity = self._compute_feature_similarity(task_tech, tool_tech)
            score += tech_similarity * 0.3
            details["tech_similarity"] = tech_similarity
            confidence += 0.3
        
        # å½’ä¸€åŒ–
        if confidence > 0:
            score = score / confidence
            confidence = min(confidence, 1.0)
        
        explanation = f"ç»“æ„ç›¸ä¼¼æ€§åŸºäºå¤æ‚åº¦({details.get('complexity_similarity', 0):.3f})ã€ä¾èµ–({details.get('dependency_similarity', 0):.3f})å’ŒæŠ€æœ¯æ ˆåŒ¹é…({details.get('tech_similarity', 0):.3f})"
        
        return SimilarityScore(
            tool_id=tool.tool_id,
            task_id=task_requirement.task_id,
            similarity_type=SimilarityType.STRUCTURAL,
            score=score,
            confidence=confidence,
            details=details,
            explanation=explanation
        )
    
    async def _compute_contextual_similarity(self, task_features: FeatureVector,
                                           tool_features: FeatureVector,
                                           tool: MCPTool,
                                           task_requirement: TaskRequirement) -> SimilarityScore:
        """è®¡ç®—ä¸Šä¸‹æ–‡ç›¸ä¼¼æ€§"""
        score = 0.5  # é»˜è®¤ä¸­ç­‰ç›¸ä¼¼æ€§
        confidence = 0.3
        details = {}
        
        # ç¯å¢ƒåŒ¹é…
        task_env = {k: v for k, v in task_features.features.items() if k.startswith("environment_")}
        tool_env = {k: v for k, v in tool_features.features.items() if k.startswith("platform_")}
        
        if task_env and tool_env:
            env_similarity = self._compute_feature_similarity(task_env, tool_env)
            score = env_similarity
            details["environment_similarity"] = env_similarity
            confidence = 0.8
        
        explanation = f"ä¸Šä¸‹æ–‡ç›¸ä¼¼æ€§åŸºäºç¯å¢ƒåŒ¹é…({details.get('environment_similarity', 0.5):.3f})"
        
        return SimilarityScore(
            tool_id=tool.tool_id,
            task_id=task_requirement.task_id,
            similarity_type=SimilarityType.CONTEXTUAL,
            score=score,
            confidence=confidence,
            details=details,
            explanation=explanation
        )
    
    async def _compute_behavioral_similarity(self, task_features: FeatureVector,
                                           tool_features: FeatureVector,
                                           tool: MCPTool,
                                           task_requirement: TaskRequirement) -> SimilarityScore:
        """è®¡ç®—è¡Œä¸ºç›¸ä¼¼æ€§"""
        score = 0.5  # é»˜è®¤ä¸­ç­‰ç›¸ä¼¼æ€§
        confidence = 0.2
        details = {}
        
        # ç®€åŒ–çš„è¡Œä¸ºç›¸ä¼¼æ€§è®¡ç®—
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œå¯ä»¥åŸºäºå·¥å…·çš„ä½¿ç”¨æ¨¡å¼ã€æ€§èƒ½ç‰¹å¾ç­‰
        
        explanation = "è¡Œä¸ºç›¸ä¼¼æ€§åŸºäºå·¥å…·ä½¿ç”¨æ¨¡å¼å’Œæ€§èƒ½ç‰¹å¾"
        
        return SimilarityScore(
            tool_id=tool.tool_id,
            task_id=task_requirement.task_id,
            similarity_type=SimilarityType.BEHAVIORAL,
            score=score,
            confidence=confidence,
            details=details,
            explanation=explanation
        )
    
    def _compute_feature_similarity(self, features1: Dict[str, float], 
                                  features2: Dict[str, float]) -> float:
        """è®¡ç®—ç‰¹å¾ç›¸ä¼¼æ€§"""
        if not features1 or not features2:
            return 0.0
        
        # è·å–æ‰€æœ‰ç‰¹å¾é”®
        all_keys = set(features1.keys()) | set(features2.keys())
        
        if not all_keys:
            return 0.0
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼æ€§
        dot_product = 0.0
        norm1 = 0.0
        norm2 = 0.0
        
        for key in all_keys:
            val1 = features1.get(key, 0.0)
            val2 = features2.get(key, 0.0)
            
            dot_product += val1 * val2
            norm1 += val1 * val1
            norm2 += val2 * val2
        
        if norm1 == 0.0 or norm2 == 0.0:
            return 0.0
        
        similarity = dot_product / (math.sqrt(norm1) * math.sqrt(norm2))
        return max(0.0, similarity)
    
    def _estimate_tool_complexity(self, tool: MCPTool) -> float:
        """ä¼°ç®—å·¥å…·å¤æ‚åº¦"""
        complexity = 0.3  # åŸºç¡€å¤æ‚åº¦
        
        # åŸºäºèƒ½åŠ›æ•°é‡
        complexity += min(len(tool.capabilities) * 0.1, 0.4)
        
        # åŸºäºä¾èµ–æ•°é‡
        if tool.dependencies:
            complexity += min(len(tool.dependencies) * 0.05, 0.2)
        
        # åŸºäºç±»åˆ«
        category_complexity = {
            ToolCategory.UTILITY: 0.2,
            ToolCategory.DEVELOPMENT: 0.6,
            ToolCategory.DATA_PROCESSING: 0.7,
            ToolCategory.WEB_AUTOMATION: 0.8,
            ToolCategory.AI_ML: 0.9
        }
        
        complexity += category_complexity.get(tool.category, 0.5) * 0.1
        
        return min(complexity, 1.0)
    
    async def _generate_matching_result(self, tool: MCPTool,
                                      task_requirement: TaskRequirement,
                                      similarity_scores: Dict[SimilarityType, SimilarityScore],
                                      strategy: MatchingStrategy) -> MatchingResult:
        """ç”ŸæˆåŒ¹é…ç»“æœ"""
        # è®¡ç®—æ€»ä½“ç›¸ä¼¼æ€§
        overall_similarity = 0.0
        total_weight = 0.0
        similarity_breakdown = {}
        
        for sim_type, score in similarity_scores.items():
            weight = self.similarity_weights.get(sim_type.value, 0.2)
            overall_similarity += score.score * weight * score.confidence
            total_weight += weight * score.confidence
            similarity_breakdown[sim_type] = score.score
        
        if total_weight > 0:
            overall_similarity = overall_similarity / total_weight
        
        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = sum(score.confidence for score in similarity_scores.values()) / len(similarity_scores)
        
        # ç”Ÿæˆè§£é‡Š
        explanation = self._generate_matching_explanation(tool, task_requirement, similarity_scores)
        
        # è¯†åˆ«åŒ¹é…å’Œç¼ºå¤±çš„ç‰¹å¾
        matching_features, missing_features = self._identify_feature_matches(
            tool, task_requirement, similarity_scores
        )
        
        return MatchingResult(
            tool_id=tool.tool_id,
            task_id=task_requirement.task_id,
            overall_similarity=overall_similarity,
            similarity_breakdown=similarity_breakdown,
            confidence=confidence,
            explanation=explanation,
            matching_features=matching_features,
            missing_features=missing_features,
            metadata={
                "strategy": strategy.value,
                "tool_name": tool.name,
                "tool_category": tool.category.value,
                "computation_time": datetime.now().isoformat()
            }
        )
    
    def _generate_matching_explanation(self, tool: MCPTool,
                                     task_requirement: TaskRequirement,
                                     similarity_scores: Dict[SimilarityType, SimilarityScore]) -> str:
        """ç”ŸæˆåŒ¹é…è§£é‡Š"""
        explanations = []
        
        # è·å–æœ€é«˜çš„ç›¸ä¼¼æ€§ç±»å‹
        best_similarity = max(similarity_scores.items(), key=lambda x: x[1].score)
        best_type, best_score = best_similarity
        
        explanations.append(f"æœ€ä½³åŒ¹é…ç»´åº¦ï¼š{best_type.value}ï¼ˆ{best_score.score:.3f}ï¼‰")
        
        # æ·»åŠ å…·ä½“çš„åŒ¹é…åŸå› 
        if best_score.explanation:
            explanations.append(best_score.explanation)
        
        # æ·»åŠ å·¥å…·ç‰¹è‰²
        if tool.capabilities:
            top_capabilities = [cap.name for cap in tool.capabilities[:3]]
            explanations.append(f"ä¸»è¦èƒ½åŠ›ï¼š{', '.join(top_capabilities)}")
        
        return "ï¼›".join(explanations)
    
    def _identify_feature_matches(self, tool: MCPTool,
                                task_requirement: TaskRequirement,
                                similarity_scores: Dict[SimilarityType, SimilarityScore]) -> Tuple[List[str], List[str]]:
        """è¯†åˆ«ç‰¹å¾åŒ¹é…"""
        matching_features = []
        missing_features = []
        
        # åŸºäºèƒ½åŠ›åŒ¹é…
        if task_requirement.required_capabilities:
            tool_capability_names = {cap.name for cap in tool.capabilities}
            
            for req_cap in task_requirement.required_capabilities:
                if req_cap in tool_capability_names:
                    matching_features.append(f"èƒ½åŠ›ï¼š{req_cap}")
                else:
                    missing_features.append(f"èƒ½åŠ›ï¼š{req_cap}")
        
        # åŸºäºç±»åˆ«åŒ¹é…
        if tool.category:
            matching_features.append(f"ç±»åˆ«ï¼š{tool.category.value}")
        
        return matching_features, missing_features
    
    async def find_similar_tools(self, reference_tool: MCPTool,
                               candidate_tools: List[MCPTool],
                               similarity_threshold: float = None) -> List[MatchingResult]:
        """æŸ¥æ‰¾ç›¸ä¼¼å·¥å…·"""
        if similarity_threshold is None:
            similarity_threshold = self.similarity_threshold
        
        # åˆ›å»ºè™šæ‹Ÿä»»åŠ¡éœ€æ±‚
        task_requirement = TaskRequirement(
            task_id=f"similarity_search_{reference_tool.tool_id}",
            description=reference_tool.description,
            required_capabilities=[cap.name for cap in reference_tool.capabilities]
        )
        
        # è®¡ç®—ç›¸ä¼¼æ€§
        matching_results = await self.compute_similarity(
            task_requirement=task_requirement,
            tools=candidate_tools,
            similarity_type=SimilarityType.HYBRID
        )
        
        # è¿‡æ»¤ç»“æœ
        similar_tools = [
            result for result in matching_results
            if result.overall_similarity >= similarity_threshold and result.tool_id != reference_tool.tool_id
        ]
        
        return similar_tools
    
    async def get_similarity_analytics(self) -> Dict[str, Any]:
        """è·å–ç›¸ä¼¼æ€§åˆ†æ"""
        return {
            "feature_cache_size": len(self.feature_cache),
            "embedding_cache_size": len(self.embedding_cache),
            "similarity_cache_size": len(self.similarity_cache),
            "similarity_weights": self.similarity_weights,
            "similarity_threshold": self.similarity_threshold,
            "confidence_threshold": self.confidence_threshold,
            "feature_extractors": list(self.feature_extractors.keys())
        }

# å·¥å‚å‡½æ•°
def get_similarity_matcher(config_path: str = "./similarity_matcher_config.json") -> SimilarityMatcher:
    """è·å–ç›¸ä¼¼æ€§åŒ¹é…å™¨å®ä¾‹"""
    return SimilarityMatcher(config_path)

# æµ‹è¯•å’Œæ¼”ç¤º
if __name__ == "__main__":
    async def test_similarity_matcher():
        """æµ‹è¯•ç›¸ä¼¼æ€§åŒ¹é…å™¨"""
        matcher = get_similarity_matcher()
        
        # åˆ›å»ºæµ‹è¯•å·¥å…·
        test_tools = [
            MCPTool(
                tool_id="file_manager",
                name="File Manager",
                description="Manage files and directories with advanced operations",
                capabilities=[
                    ToolCapability(name="file_operations", confidence=0.9),
                    ToolCapability(name="directory_management", confidence=0.8)
                ],
                category=ToolCategory.UTILITY
            ),
            MCPTool(
                tool_id="web_scraper",
                name="Web Scraper",
                description="Extract data from websites using intelligent parsing",
                capabilities=[
                    ToolCapability(name="web_scraping", confidence=0.95),
                    ToolCapability(name="data_extraction", confidence=0.8)
                ],
                category=ToolCategory.WEB_AUTOMATION
            ),
            MCPTool(
                tool_id="data_processor",
                name="Data Processor",
                description="Process and analyze large datasets efficiently",
                capabilities=[
                    ToolCapability(name="data_processing", confidence=0.9),
                    ToolCapability(name="data_analysis", confidence=0.85)
                ],
                category=ToolCategory.DATA_PROCESSING
            )
        ]
        
        # åˆ›å»ºä»»åŠ¡éœ€æ±‚
        task_requirement = TaskRequirement(
            task_id="test_task",
            description="Extract data from websites and save to files for analysis",
            required_capabilities=["web_scraping", "file_operations", "data_processing"],
            complexity_level="medium"
        )
        
        # è®¡ç®—ç›¸ä¼¼æ€§
        print("ğŸ” è®¡ç®—å·¥å…·ç›¸ä¼¼æ€§...")
        matching_results = await matcher.compute_similarity(
            task_requirement=task_requirement,
            tools=test_tools,
            similarity_type=SimilarityType.HYBRID
        )
        
        print(f"ğŸ“Š åŒ¹é…ç»“æœ ({len(matching_results)} ä¸ª):")
        for result in matching_results:
            tool = next(t for t in test_tools if t.tool_id == result.tool_id)
            print(f"  {result.rank}. {tool.name}")
            print(f"     æ€»ä½“ç›¸ä¼¼æ€§: {result.overall_similarity:.3f}")
            print(f"     ç½®ä¿¡åº¦: {result.confidence:.3f}")
            print(f"     è§£é‡Š: {result.explanation}")
            print(f"     åŒ¹é…ç‰¹å¾: {', '.join(result.matching_features)}")
            if result.missing_features:
                print(f"     ç¼ºå¤±ç‰¹å¾: {', '.join(result.missing_features)}")
            print()
        
        # æŸ¥æ‰¾ç›¸ä¼¼å·¥å…·
        if test_tools:
            print("ğŸ”— æŸ¥æ‰¾ç›¸ä¼¼å·¥å…·...")
            similar_tools = await matcher.find_similar_tools(
                reference_tool=test_tools[0],
                candidate_tools=test_tools[1:],
                similarity_threshold=0.3
            )
            
            print(f"ç›¸ä¼¼å·¥å…· ({len(similar_tools)} ä¸ª):")
            for result in similar_tools:
                tool = next(t for t in test_tools if t.tool_id == result.tool_id)
                print(f"  - {tool.name} (ç›¸ä¼¼æ€§: {result.overall_similarity:.3f})")
        
        # è·å–åˆ†æ
        analytics = await matcher.get_similarity_analytics()
        print(f"ğŸ“ˆ ç›¸ä¼¼æ€§åˆ†æ:")
        print(f"  ç‰¹å¾ç¼“å­˜: {analytics['feature_cache_size']}")
        print(f"  åµŒå…¥ç¼“å­˜: {analytics['embedding_cache_size']}")
        print(f"  ç›¸ä¼¼æ€§é˜ˆå€¼: {analytics['similarity_threshold']}")
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_similarity_matcher())

